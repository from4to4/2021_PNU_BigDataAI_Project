{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T14:50:06.358706Z",
     "start_time": "2021-05-15T14:50:06.353719Z"
    }
   },
   "outputs": [],
   "source": [
    "# soup 요청 함수\n",
    "def getSource(site) :\n",
    "    \n",
    "    import requests\n",
    "    import bs4\n",
    "    \n",
    "    # 헤더 정보\n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb Kit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # 요청한다.\n",
    "    response = requests.get(site, headers=header_info)\n",
    "    \n",
    "    # bs4 객체 생성\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T14:50:06.644329Z",
     "start_time": "2021-05-15T14:50:06.639325Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한 페이지에 있는 다음 뉴스 링크 수집 함수\n",
    "def getNewsLink(site, COLOPHON, KEYWORD):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    soup = getSource(site)\n",
    "\n",
    "    link_list = []\n",
    "\n",
    "    # li 태그 가져오기\n",
    "    a1 = soup.select('#clusterResultUL > li')\n",
    "    # print(len(a1))\n",
    "    \n",
    "    for a2 in a1:\n",
    "        \n",
    "        # div 태그 가져오기\n",
    "        a3 = a2.select_one('div.wrap_cont > div > div > a')\n",
    "        # print(a3)\n",
    "        \n",
    "        # 기사링크 \n",
    "        data1 = a3.attrs['href']\n",
    "        # print(data1)\n",
    "        \n",
    "        # 기사제목\n",
    "        data2 = a3.text.strip()\n",
    "        # print(data2)\n",
    "    \n",
    "        # span 태그 가져오기\n",
    "        a4 = a2.select_one('div.wrap_cont > div > span.f_nb.date')\n",
    "        a5 = a4.text.strip().split('|')\n",
    "        \n",
    "        # 날짜\n",
    "        data3 = a5[0]\n",
    "        \n",
    "        # 언론사\n",
    "        data4 = a5[1]\n",
    "        \n",
    "        # print(data1, data2, data3, data4)\n",
    "        \n",
    "        # 기사 링크 리스트에 저장\n",
    "        link_list.append(data1)\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    df1 = pd.DataFrame(link_list)\n",
    "    # display(df1)\n",
    "    \n",
    "\n",
    "    FILENAME = f'{COLOPHON}_{KEYWORD}_link.csv'\n",
    "\n",
    "    if os.path.exists(FILENAME) == False:\n",
    "        # 파일이 없을 경우\n",
    "        df1.to_csv(FILENAME, encoding='utf-8-sig', index=False)\n",
    "    else:\n",
    "        # mode='a' : 기존 것 뒤에다 붙여줌\n",
    "        df1.to_csv(FILENAME, encoding='utf-8-sig', index=False, header=False, mode='a')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T14:50:06.913796Z",
     "start_time": "2021-05-15T14:50:06.908809Z"
    }
   },
   "outputs": [],
   "source": [
    "# 다음 페이지 존재 여부 확인하는 함수\n",
    "def getNextPage(site) :\n",
    "    \n",
    "    # url에서 p= 값 들고오기\n",
    "    p = site.split('&')[-1].split('=')[-1]\n",
    "    \n",
    "    # p값에 1 더해서 다음 페이지 url 만들기\n",
    "    nextPage = site[:-len(p)] + str(int(p)+1)\n",
    "    # print(next_page)\n",
    "\n",
    "    # 현재 페이지와 다음 페이지 soup 가져오기\n",
    "    soup1 = getSource(site)\n",
    "    soup2 = getSource(nextPage)\n",
    "    # print(soup1)\n",
    "    # print(soup2)\n",
    "\n",
    "    # 현재 페이지와 다음 페이지 첫번째 a 태그에서 링크 가져오기\n",
    "    a1 = soup1.select('#clusterResultUL > li > div.wrap_cont > div > div > a')[0].attrs['href']\n",
    "    a2 = soup2.select('#clusterResultUL > li > div.wrap_cont > div > div > a')[0].attrs['href']\n",
    "    # print(a1)\n",
    "    # print(a2)\n",
    "   \n",
    "    # 두 링크가 같지 않으면 다음 페이지가 있다고 간주, 다음 페이지 return \n",
    "    if a1 != a2 :\n",
    "        return True\n",
    "    # 같으면 다음 페이지 없다고 간주, False return \n",
    "    else :\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T14:50:07.278764Z",
     "start_time": "2021-05-15T14:50:07.271782Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDaumNewsUrlDF(KEYWORD, COLOPHON, dayStart, dayEnd, page):\n",
    "\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from IPython.display import clear_output\n",
    "    import urllib\n",
    "    \n",
    "    # 파일명을 위해\n",
    "    KEYWORD_text = KEYWORD\n",
    "\n",
    "    # 검색어\n",
    "    KEYWORD = urllib.parse.quote(KEYWORD)\n",
    "    COLOPHON = COLOPHON\n",
    "    dayStart = dayStart\n",
    "    dayEnd = dayEnd\n",
    "    page=page\n",
    "    \n",
    "    # 다음 뉴스 검색 url\n",
    "    URL = 'https://search.daum.net/search?w=news&enc=utf8&cluster=y&cluster_page=1&'\n",
    "    \n",
    "    # url에 들어갈 파라미터\n",
    "    cp_dict = {'조선일보' : '16d4PV266g2j-N3GYq',\n",
    "               '중앙일보' : '16Elf9uX5H6T5xXvQV',\n",
    "               '동아일보' : '16bOiOx4gG2S18EPLj',\n",
    "               'JTBC'     : '16yZfDfR_rGcw5F-P0',\n",
    "               '경향신문' : '16akMkKFDu6n8GTzZr',\n",
    "               '한겨레' : '16nzyJHdH5ORpabfqG'}\n",
    "    cpName = urllib.parse.quote(COLOPHON)\n",
    "    cp = cp_dict[COLOPHON]\n",
    "    \n",
    "    # 페이지 번호\n",
    "    page = 1\n",
    "    \n",
    "    while True :\n",
    "        time.sleep(1)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        site = f'{URL}q={KEYWORD}&cpname={cpName}&cp={cp}&period=6m&sd={dayStart}&ed={dayEnd}&DA=PGD&p={page}'\n",
    "\n",
    "        print(f' 다음 뉴스 - {COLOPHON} : {page} 페이지 수집 중' )\n",
    "\n",
    "        getNewsLink(site, COLOPHON, KEYWORD_text ) \n",
    "        chk = getNextPage(site)\n",
    "\n",
    "        if chk != False:\n",
    "            page = page + 1\n",
    "        else: \n",
    "            print(f'{COLOPHON}_{KEYWORD_text}_link.csv 파일 저장 완료')\n",
    "            break\n",
    "    \n",
    "    df = pd.read_csv(f'{COLOPHON}_{KEYWORD_text}_link.csv', )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 다음 뉴스 - 조선일보 : 3 페이지 수집 중\n",
      "조선일보_대선 김부겸_link.csv 파일 저장 완료\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://v.media.daum.net/v/20210507105714648?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://v.media.daum.net/v/20210507005304224?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://v.media.daum.net/v/20210503221519290?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://v.media.daum.net/v/20210512035100311?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://v.media.daum.net/v/20210513031150577?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://v.media.daum.net/v/20210513030221314?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://v.media.daum.net/v/20210510032539992?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://v.media.daum.net/v/20210416134435200?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://v.media.daum.net/v/20210416000918402?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://v.media.daum.net/v/20210416233953906?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://v.media.daum.net/v/20210503030528073?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://v.media.daum.net/v/20210420000416250?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://v.media.daum.net/v/20210423032505382?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://v.media.daum.net/v/20210417030345062?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://v.media.daum.net/v/20210404095416811?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>http://v.media.daum.net/v/20210423033325407?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>http://v.media.daum.net/v/20210419030407986?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>http://v.media.daum.net/v/20210418151831030?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>http://v.media.daum.net/v/20210416181635789?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>http://v.media.daum.net/v/20210417154953557?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>http://v.media.daum.net/v/20210407032521545?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>http://v.media.daum.net/v/20210309030738007?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>http://v.media.daum.net/v/20210309013527338?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>http://v.media.daum.net/v/20210301031038728?f=o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0\n",
       "0   http://v.media.daum.net/v/20210507105714648?f=o\n",
       "1   http://v.media.daum.net/v/20210507005304224?f=o\n",
       "2   http://v.media.daum.net/v/20210503221519290?f=o\n",
       "3   http://v.media.daum.net/v/20210512035100311?f=o\n",
       "4   http://v.media.daum.net/v/20210513031150577?f=o\n",
       "5   http://v.media.daum.net/v/20210513030221314?f=o\n",
       "6   http://v.media.daum.net/v/20210510032539992?f=o\n",
       "7   http://v.media.daum.net/v/20210416134435200?f=o\n",
       "8   http://v.media.daum.net/v/20210416000918402?f=o\n",
       "9   http://v.media.daum.net/v/20210416233953906?f=o\n",
       "10  http://v.media.daum.net/v/20210503030528073?f=o\n",
       "11  http://v.media.daum.net/v/20210420000416250?f=o\n",
       "12  http://v.media.daum.net/v/20210423032505382?f=o\n",
       "13  http://v.media.daum.net/v/20210417030345062?f=o\n",
       "14  http://v.media.daum.net/v/20210404095416811?f=o\n",
       "15  http://v.media.daum.net/v/20210423033325407?f=o\n",
       "16  http://v.media.daum.net/v/20210419030407986?f=o\n",
       "17  http://v.media.daum.net/v/20210418151831030?f=o\n",
       "18  http://v.media.daum.net/v/20210416181635789?f=o\n",
       "19  http://v.media.daum.net/v/20210417154953557?f=o\n",
       "20  http://v.media.daum.net/v/20210407032521545?f=o\n",
       "21  http://v.media.daum.net/v/20210309030738007?f=o\n",
       "22  http://v.media.daum.net/v/20210309013527338?f=o\n",
       "23  http://v.media.daum.net/v/20210301031038728?f=o"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = True\n",
    "\n",
    "names = ['윤석열', '이재명','이낙연', '홍준표', '안철수', '정세균', '유승민', '심상정', '추미애', '황교안', '김부겸']\n",
    "\n",
    "if test:\n",
    "    for name in names:    \n",
    "        # 검색어\n",
    "        KEYWORD = f'대선 {name}'\n",
    "\n",
    "        # 언론\n",
    "        # 조선일보, 중앙일보, 동아일보, JTBC, 경향신문, 한겨레 택1\n",
    "        COLOPHON = '중앙일보'\n",
    "\n",
    "        # 날짜 (YYYYMMDhhmmss)\n",
    "        dayStart = '20210301000000'\n",
    "        dayEnd   = '20210514000000'\n",
    "        df = getDaumNewsUrlDF(KEYWORD=KEYWORD, COLOPHON=COLOPHON, dayStart=dayStart, dayEnd=dayEnd, page=1)\n",
    "        display(df)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "6bd18ff4b8d78789dc0c4fe3ce3d1752f57bd4a78312d15f34612069d81af063"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
