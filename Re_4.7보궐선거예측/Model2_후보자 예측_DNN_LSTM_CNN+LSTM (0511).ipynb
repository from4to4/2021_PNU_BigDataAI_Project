{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델2\n",
    "- 텍스트를 보고 후보자 예측\n",
    "- 입력 데이터(Feature)\n",
    "    - 기사제목+댓글내용\n",
    "- 출력 데이터(Label)\n",
    "    - 박영선\n",
    "    - 오세훈\n",
    "    - 김영춘\n",
    "    - 박형준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:21.314087Z",
     "start_time": "2021-05-11T15:18:17.137961Z"
    }
   },
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import missingno\n",
    "import re\n",
    "import os\n",
    "# 데이터\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 경고메시지 제거\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# KFold\n",
    "from sklearn.model_selection import KFold # 순서대로 하거나 or 랜덤하게 클래스를 나눈다.\n",
    "from sklearn.model_selection import StratifiedKFold # 결과데이터를 보고 각 클래스가 균등한 비율로 들어있게끔 나눈다.\n",
    "\n",
    "# 교차검증 함수\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 학습데이터와 검증데이터로 나누는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 전처리\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# 하이퍼 파라미터 튜닝\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "    \n",
    "# 머신러닝 알고리즘 - 분류\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# 추가항목\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 머신러닝 알고리즘 - 회귀\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 비지도학습 - 군집\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import mean_shift\n",
    "\n",
    "# 딥러닝\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "# CNN\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, Conv1D\n",
    "from keras.layers import MaxPooling2D, MaxPooling1D\n",
    "\n",
    "# NLP\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# 문장을 단어 단위로 나누기\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# 다중분류를 위한 원-핫 인코더\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 더이상 성능 향상이 이루어지지 않는다면 조기 중단시킬 수 있는 함수\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# epoch마다 모델을 저장하는 함수\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# 파일로 저장된 딥러닝 모델을 객체로 복구하는 함수\n",
    "from keras.models import load_model\n",
    "\n",
    "# 저장\n",
    "import pickle\n",
    "\n",
    "# 시간 모듈\n",
    "import time # 현재 시간값을 구할 수 있다.\n",
    "\n",
    "# 그래프 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['font.size'] = 13\n",
    "plt.rcParams['figure.figsize'] = 10,5\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:22.188803Z",
     "start_time": "2021-05-11T15:18:21.315085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# GPU 메모리 사용량을 필요한 만큼만 증가하도록 설정\n",
    "\n",
    "# 사용가능한 GPU 목록 가져오기\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        # 필요한 만큼만 메모리를 사용할 수 있도록 설정하기\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:22.205757Z",
     "start_time": "2021-05-11T15:18:22.189801Z"
    }
   },
   "outputs": [],
   "source": [
    "# RandomSeed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:24.259161Z",
     "start_time": "2021-05-11T15:18:22.206757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144139, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>날짜</th>\n",
       "      <th>작성일</th>\n",
       "      <th>댓글</th>\n",
       "      <th>출처</th>\n",
       "      <th>지역(서울:1, 부산:2)</th>\n",
       "      <th>정당(1:더불어민주당,2:국민의힘)</th>\n",
       "      <th>정당평가(부정;0, 긍정:1)</th>\n",
       "      <th>후보(기호 순)</th>\n",
       "      <th>후보평가(부정;0, 긍정:1)</th>\n",
       "      <th>제목댓글</th>\n",
       "      <th>전처리 제목댓글</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-12 19:13:00</td>\n",
       "      <td>철수야! 뜸 들이지 말고 애국하는 마음으로 물러서라~~~</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 철수야! 뜸 들이...</td>\n",
       "      <td>재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 철수야 뜸 들이지 말고 애...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-09 13:49:00</td>\n",
       "      <td>박영선은 정동영이 얻은 36프로선에 머무를것. 4.7.이후 OOO정권은 몰락의 길 ...</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 박영선은 정동영이...</td>\n",
       "      <td>재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 박영선은 정동영이 얻은 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-07 14:42:00</td>\n",
       "      <td>빵선이가서울시장되면서울은공산국가수도제2의평양이될것이다</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 빵선이가서울시장되...</td>\n",
       "      <td>재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 빵 산 이가 서울시장 되면...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-07 14:38:00</td>\n",
       "      <td>서울시장후보더듬당박빵선이는절대로서울시장을할수없다이유는가족은미국.영국에 영주권자이므로...</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 서울시장후보더듬당...</td>\n",
       "      <td>재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 서울시장 후보 더 든 대치...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?</td>\n",
       "      <td>2021-03-07 05:57:00</td>\n",
       "      <td>2021-03-07 14:02:00</td>\n",
       "      <td>부산은오거돈선거이고 오거돈치부선거아닌가 오거돈에 성추해으로 생긴선거가 가독도신공항은...</td>\n",
       "      <td>조선일보</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 부산은오거돈선거이...</td>\n",
       "      <td>재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 부산은 오거돈 선거이고 오...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     제목                   날짜  \\\n",
       "0  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "1  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "2  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "3  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "4  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결?  2021-03-07 05:57:00   \n",
       "\n",
       "                   작성일                                                 댓글  \\\n",
       "0  2021-03-12 19:13:00                    철수야! 뜸 들이지 말고 애국하는 마음으로 물러서라~~~   \n",
       "1  2021-03-09 13:49:00  박영선은 정동영이 얻은 36프로선에 머무를것. 4.7.이후 OOO정권은 몰락의 길 ...   \n",
       "2  2021-03-07 14:42:00                      빵선이가서울시장되면서울은공산국가수도제2의평양이될것이다   \n",
       "3  2021-03-07 14:38:00  서울시장후보더듬당박빵선이는절대로서울시장을할수없다이유는가족은미국.영국에 영주권자이므로...   \n",
       "4  2021-03-07 14:02:00  부산은오거돈선거이고 오거돈치부선거아닌가 오거돈에 성추해으로 생긴선거가 가독도신공항은...   \n",
       "\n",
       "     출처  지역(서울:1, 부산:2)  정당(1:더불어민주당,2:국민의힘)  정당평가(부정;0, 긍정:1)  후보(기호 순)  \\\n",
       "0  조선일보             NaN                  NaN               NaN       NaN   \n",
       "1  조선일보             1.0                  NaN               NaN       1.0   \n",
       "2  조선일보             1.0                  NaN               NaN       1.0   \n",
       "3  조선일보             1.0                  1.0               0.0       1.0   \n",
       "4  조선일보             2.0                  1.0               0.0       NaN   \n",
       "\n",
       "   후보평가(부정;0, 긍정:1)                                               제목댓글  \\\n",
       "0               NaN  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 철수야! 뜸 들이...   \n",
       "1               0.0  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 박영선은 정동영이...   \n",
       "2               0.0  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 빵선이가서울시장되...   \n",
       "3               0.0  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 서울시장후보더듬당...   \n",
       "4               NaN  [재보궐 D-31] 부산 김영춘 vs 박형준, 서울도 양자 대결? 부산은오거돈선거이...   \n",
       "\n",
       "                                            전처리 제목댓글  \n",
       "0  재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 철수야 뜸 들이지 말고 애...  \n",
       "1  재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 박영선은 정동영이 얻은 3...  \n",
       "2  재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 빵 산 이가 서울시장 되면...  \n",
       "3  재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 서울시장 후보 더 든 대치...  \n",
       "4  재보궐 D31 부산 김영춘 vs 박형준 서울도 양자 대결 부산은 오거돈 선거이고 오...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/재보궐선거댓글데이터_최종_전처리완료.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "- 후보자별 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:24.319033Z",
     "start_time": "2021-05-11T15:18:24.260159Z"
    }
   },
   "outputs": [],
   "source": [
    "# 사용할 데이터\n",
    "df2 = df[['지역(서울:1, 부산:2)', '후보(기호 순)', '후보평가(부정;0, 긍정:1)',\n",
    "          '정당(1:더불어민주당,2:국민의힘)', '정당평가(부정;0, 긍정:1)', '전처리 제목댓글']]\n",
    "df2.columns = ['area', 'candidate', 'candidate_eval',\n",
    "               'party', 'party_eval', 'title_comment']\n",
    "\n",
    "# 후보자를 구별하여 'area_candidate' 컬럼에 각 후보자 이름 추가\n",
    "\n",
    "# 후보자별 인덱스 추출\n",
    "ys_idx = df2.query('area == 1.0 & candidate == 1.0').index  # 박영선\n",
    "sh_idx = df2.query('area == 1.0 & candidate == 2.0').index  # 오세훈\n",
    "yc_idx = df2.query('area == 2.0 & candidate == 1.0').index  # 김영춘\n",
    "hj_idx = df2.query('area == 2.0 & candidate == 2.0').index  # 박형준\n",
    "etc_idx = df2.query('candidate == 5.0').index  # 기타\n",
    "\n",
    "# 'area_candidate'컬럼에 후보자 이름값 추가\n",
    "df2['area_candidate'] = np.nan\n",
    "df2['area_candidate'][ys_idx] = '박영선'\n",
    "df2['area_candidate'][sh_idx] = '오세훈'\n",
    "df2['area_candidate'][yc_idx] = '김영춘'\n",
    "df2['area_candidate'][hj_idx] = '박형준'\n",
    "df2['area_candidate'][etc_idx] = '기타'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:24.342938Z",
     "start_time": "2021-05-11T15:18:24.320001Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 라벨링된 데이터수: 4332\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>기타</th>\n",
       "      <td>1611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>박영선</th>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>오세훈</th>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>박형준</th>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>김영춘</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     area_candidate\n",
       "기타             1611\n",
       "박영선            1303\n",
       "오세훈            1178\n",
       "박형준             211\n",
       "김영춘              29"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 종류별 개수 확인\n",
    "print(f'총 라벨링된 데이터수: {df2[\"area_candidate\"].value_counts().sum()}')\n",
    "pd.DataFrame(df2['area_candidate'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 다중분류\n",
    "- **활성화 함수**를 **이용**하려면 Y값이 **0-1로 이루어져** 있어야 함\n",
    "    - 원-핫 인코딩(One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 학습할 텍스트(docs) 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:24.348922Z",
     "start_time": "2021-05-11T15:18:24.344932Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한글 정규표현식\n",
    "def text_cleaning(text) :\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    result = hangul.sub('', str(text))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:24.963308Z",
     "start_time": "2021-05-11T15:18:24.350916Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한글 외 문자열 공백처리\n",
    "df2['title_comment'] = df2['title_comment'].apply(lambda x: text_cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:24.987248Z",
     "start_time": "2021-05-11T15:18:24.964302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4332"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 후보자를 5가지로 분리가능한 데이터수\n",
    "notnull_idx = df2[ df2['area_candidate'].notnull() ].index\n",
    "\n",
    "# 후보자 예측에 사용될 feature 텍스트 선정\n",
    "docs = df2['title_comment'][notnull_idx].to_list()\n",
    "len(docs) # 총 라벨링된 데이터수랑 같아야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  y값 문자열 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:24.995194Z",
     "start_time": "2021-05-11T15:18:24.988212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 4, 0, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자열 인코딩\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# 후보값을 LabelEncoding\n",
    "y = encoder.fit_transform(df2['area_candidate'][notnull_idx].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:25.003172Z",
     "start_time": "2021-05-11T15:18:24.996192Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기타' '김영춘' '박영선' '박형준' '오세훈']\n",
      "['기타']\n",
      "['김영춘']\n",
      "['박영선']\n",
      "['박형준']\n",
      "['오세훈']\n"
     ]
    }
   ],
   "source": [
    "# 인코딩값 확인\n",
    "print( encoder.classes_ )\n",
    "print( encoder.inverse_transform([0]) )\n",
    "print( encoder.inverse_transform([1]) )\n",
    "print( encoder.inverse_transform([2]) )\n",
    "print( encoder.inverse_transform([3]) )\n",
    "print( encoder.inverse_transform([4]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  y값 One-Hot Encoding\n",
    "- 출력층 node의 개수는 **5개**로 맞춰주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:25.010153Z",
     "start_time": "2021-05-11T15:18:25.005169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4332, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded = to_categorical(y)\n",
    "y_encoded.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:18:25.022122Z",
     "start_time": "2021-05-11T15:18:25.011151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded # ⭐출력층 노드의 개수는 5로 맞춰주기⭐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Tokenizer\n",
    "- 텍스트를 잘게 나누는 것\n",
    "- 단어별, 문장별, 형태소별, ...\n",
    "- 이렇게 나누어진 하나의 단위를 **Token(토큰)**\n",
    "\n",
    "\n",
    "- **빈도에 따라 번호가 정해지도록 만들기**\n",
    "    - Tokenizer(num_words=5000): 빈도가 높은 토큰들 중 5000개 토큰만 선택해서 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:09.895858Z",
     "start_time": "2021-05-11T15:52:09.742651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'박영선': 1,\n",
       " '오세훈': 2,\n",
       " '다': 3,\n",
       " '안철수': 4,\n",
       " '서울시장': 5,\n",
       " '문재인': 6,\n",
       " '한': 7,\n",
       " '안': 8,\n",
       " '후보': 9,\n",
       " '이': 10,\n",
       " '보궐선거': 11,\n",
       " '못': 12,\n",
       " '내곡동': 13,\n",
       " '단일화': 14,\n",
       " '땅': 15,\n",
       " '천안함': 16,\n",
       " '이게': 17,\n",
       " '유승민': 18,\n",
       " '참석': 19,\n",
       " '추모식': 20,\n",
       " '것': 21,\n",
       " '국민의': 22,\n",
       " '더불어민주당': 23,\n",
       " '대통령': 24,\n",
       " '후보자': 25,\n",
       " '토론회': 26,\n",
       " '나라냐': 27,\n",
       " '번': 28,\n",
       " '하태경': 29,\n",
       " '수': 30,\n",
       " '있는': 31,\n",
       " '토론': 32,\n",
       " '하는': 33,\n",
       " '그': 34,\n",
       " '년': 35,\n",
       " '박형준': 36,\n",
       " '다시': 37,\n",
       " '한다': 38,\n",
       " '선거': 39,\n",
       " '주영진의': 40,\n",
       " '뉴스브리핑': 41,\n",
       " '게': 42,\n",
       " '더': 43,\n",
       " '이런': 44,\n",
       " '할': 45,\n",
       " '회': 46,\n",
       " '당신의': 47,\n",
       " '서울': 48,\n",
       " '없는': 49,\n",
       " '선택은': 50,\n",
       " '분토론': 51,\n",
       " '또': 52,\n",
       " '왜': 53,\n",
       " '원인첫': 54,\n",
       " '어땠나': 55,\n",
       " '때': 56,\n",
       " '물었다': 57,\n",
       " '부산시장': 58,\n",
       " '것이다': 59,\n",
       " '야당': 60,\n",
       " '전': 61,\n",
       " '것이': 62,\n",
       " '건': 63,\n",
       " '와': 64,\n",
       " '사설': 65,\n",
       " '김어준': 66,\n",
       " '만': 67,\n",
       " '부산': 68,\n",
       " '등': 69,\n",
       " '김영춘': 70,\n",
       " '두': 71,\n",
       " '거': 72,\n",
       " '사저': 73,\n",
       " '되면': 74,\n",
       " '오세훈이': 75,\n",
       " '지혜원의': 76,\n",
       " '사저는': 77,\n",
       " '노무현의': 78,\n",
       " '구하기': 79,\n",
       " '하고': 80,\n",
       " '아방': 81,\n",
       " '시장': 82,\n",
       " '있다': 83,\n",
       " '친구가': 84,\n",
       " '일': 85,\n",
       " '이럽니까': 86,\n",
       " '같은': 87,\n",
       " '정치': 88,\n",
       " '참': 89,\n",
       " '선언': 90,\n",
       " '좀': 91,\n",
       " '없다': 92,\n",
       " '고민정': 93,\n",
       " '정권': 94,\n",
       " '국민': 95,\n",
       " '지금': 96,\n",
       " '무슨': 97,\n",
       " '민주당': 98,\n",
       " '때문에': 99,\n",
       " '자': 100,\n",
       " '잘': 101,\n",
       " '만에': 102,\n",
       " '안철수가': 103,\n",
       " '그런': 104,\n",
       " '출마': 105,\n",
       " '후': 106,\n",
       " '될': 107,\n",
       " '합니다': 108,\n",
       " '가': 109,\n",
       " '논란': 110,\n",
       " '정말': 111,\n",
       " '수도': 112,\n",
       " '힘': 113,\n",
       " '나라': 114,\n",
       " '누가': 115,\n",
       " '문': 116,\n",
       " '초청': 117,\n",
       " '내': 118,\n",
       " '다시보기': 119,\n",
       " '사람': 120,\n",
       " '이수봉': 121,\n",
       " '인방': 122,\n",
       " '가덕도': 123,\n",
       " '하나': 124,\n",
       " '것을': 125,\n",
       " '단일후보': 126,\n",
       " '결렬': 127,\n",
       " '등록한': 128,\n",
       " '하면': 129,\n",
       " '이제': 130,\n",
       " '뉴스': 131,\n",
       " '반드시': 132,\n",
       " '그리고': 133,\n",
       " '성추행': 134,\n",
       " '박원순': 135,\n",
       " '요즘': 136,\n",
       " '결국': 137,\n",
       " '분노': 138,\n",
       " '철이': 139,\n",
       " '것은': 140,\n",
       " '김종인': 141,\n",
       " '아닌가': 142,\n",
       " '대통령님': 143,\n",
       " '쫓아내야': 144,\n",
       " '말고': 145,\n",
       " '돈': 146,\n",
       " '거다': 147,\n",
       " '대는': 148,\n",
       " '정부': 149,\n",
       " '사과에': 150,\n",
       " '뭐': 151,\n",
       " '모두': 152,\n",
       " '네': 153,\n",
       " '너무': 154,\n",
       " '여론조사': 155,\n",
       " '친': 156,\n",
       " '야권': 157,\n",
       " '원': 158,\n",
       " '되는': 159,\n",
       " '꼭': 160,\n",
       " '보면': 161,\n",
       " '집': 162,\n",
       " '개': 163,\n",
       " '줄': 164,\n",
       " '군': 165,\n",
       " '힘과': 166,\n",
       " '없어': 167,\n",
       " '그렇게': 168,\n",
       " '단독': 169,\n",
       " '나라를': 170,\n",
       " '핑계': 171,\n",
       " '사전': 172,\n",
       " '낙선': 173,\n",
       " '후보에': 174,\n",
       " '없어서': 175,\n",
       " '침묵': 176,\n",
       " '합당': 177,\n",
       " '저': 178,\n",
       " '우리': 179,\n",
       " '큰': 180,\n",
       " '당': 181,\n",
       " '양보': 182,\n",
       " '안철수는': 183,\n",
       " '위해': 184,\n",
       " '보고': 185,\n",
       " '앞서': 186,\n",
       " '선거운동': 187,\n",
       " '신공항': 188,\n",
       " '주민': 189,\n",
       " '투기': 190,\n",
       " '하지': 191,\n",
       " '이번': 192,\n",
       " '듯': 193,\n",
       " '선출': 194,\n",
       " '재도전': 195,\n",
       " '진짜': 196,\n",
       " '된다': 197,\n",
       " '그냥': 198,\n",
       " '네티즌들': 199,\n",
       " '말': 200,\n",
       " '정신': 201,\n",
       " '대': 202,\n",
       " '등록': 203,\n",
       " '걸': 204,\n",
       " '추진': 205,\n",
       " '사람이': 206,\n",
       " '아': 207,\n",
       " '이해찬': 208,\n",
       " '승패와': 209,\n",
       " '무관하게': 210,\n",
       " '캠페인': 211,\n",
       " '각자': 212,\n",
       " '주진우': 213,\n",
       " '자신': 214,\n",
       " '제대로': 215,\n",
       " '합이': 216,\n",
       " '공장': 217,\n",
       " '제': 218,\n",
       " '제발': 219,\n",
       " '수용': 220,\n",
       " '분노엔': 221,\n",
       " '좀스럽다는': 222,\n",
       " '후보가': 223,\n",
       " '해라': 224,\n",
       " '나': 225,\n",
       " '월': 226,\n",
       " '의혹엔': 227,\n",
       " '상대방': 228,\n",
       " '모든': 229,\n",
       " '고': 230,\n",
       " '동시에': 231,\n",
       " '구할': 232,\n",
       " '대한민국': 233,\n",
       " '씨': 234,\n",
       " '같다': 235,\n",
       " '이렇게': 236,\n",
       " '어떻게': 237,\n",
       " '오세훈은': 238,\n",
       " '송영길': 239,\n",
       " '없어질': 240,\n",
       " '요구': 241,\n",
       " '뽑아야': 242,\n",
       " '새로운': 243,\n",
       " '불발': 244,\n",
       " '해': 245,\n",
       " '마지막': 246,\n",
       " '아파트': 247,\n",
       " '총선': 248,\n",
       " '측': 249,\n",
       " '재협상': 250,\n",
       " '된': 251,\n",
       " '파이팅': 252,\n",
       " '많이': 253,\n",
       " '내가': 254,\n",
       " '아니라': 255,\n",
       " '국방부': 256,\n",
       " '나라가': 257,\n",
       " '은': 258,\n",
       " '중립': 259,\n",
       " '김종인에게': 260,\n",
       " '상왕이라니': 261,\n",
       " '양해': 262,\n",
       " '박영선이': 263,\n",
       " '무능': 264,\n",
       " '바로': 265,\n",
       " '해야': 266,\n",
       " '부동산': 267,\n",
       " '어찌': 268,\n",
       " '사': 269,\n",
       " '부정': 270,\n",
       " '둘': 271,\n",
       " '나경원': 272,\n",
       " '없음': 273,\n",
       " '국민이': 274,\n",
       " '데': 275,\n",
       " '오세훈의': 276,\n",
       " '후보를': 277,\n",
       " '마라': 278,\n",
       " '땐': 279,\n",
       " '얼마나': 280,\n",
       " '국민을': 281,\n",
       " '아니면': 282,\n",
       " '도대체': 283,\n",
       " '대치어': 284,\n",
       " '있습니다': 285,\n",
       " '말이': 286,\n",
       " '거의': 287,\n",
       " '더불어': 288,\n",
       " '이미': 289,\n",
       " '나는': 290,\n",
       " '간': 291,\n",
       " '좋은': 292,\n",
       " '좌파': 293,\n",
       " '여론조사박영선': 294,\n",
       " '응원합니다': 295,\n",
       " '조국': 296,\n",
       " '아주': 297,\n",
       " '나온': 298,\n",
       " '투표': 299,\n",
       " '것도': 300,\n",
       " '아니다': 301,\n",
       " '않는': 302,\n",
       " '일이': 303,\n",
       " '사람은': 304,\n",
       " '바란다': 305,\n",
       " '보니': 306,\n",
       " '아직도': 307,\n",
       " '절대': 308,\n",
       " '아닌': 309,\n",
       " '국': 310,\n",
       " '리더십': 311,\n",
       " '중': 312,\n",
       " '필요': 313,\n",
       " '있나': 314,\n",
       " '현실': 315,\n",
       " '해도': 316,\n",
       " '표': 317,\n",
       " '나쁜': 318,\n",
       " '대한': 319,\n",
       " '거지': 320,\n",
       " '만들겠다': 321,\n",
       " '실정': 322,\n",
       " '저런': 323,\n",
       " '몰라': 324,\n",
       " '봉투': 325,\n",
       " '초접전': 326,\n",
       " '거부당해': 327,\n",
       " '어떤': 328,\n",
       " '문재인이': 329,\n",
       " '당장': 330,\n",
       " '장관': 331,\n",
       " '일본': 332,\n",
       " '여론': 333,\n",
       " '의혹': 334,\n",
       " '연발': 335,\n",
       " '내민': 336,\n",
       " '않고': 337,\n",
       " '놈들': 338,\n",
       " '일은': 339,\n",
       " '청년': 340,\n",
       " '지난': 341,\n",
       " '아니고': 342,\n",
       " '수사': 343,\n",
       " '동안': 344,\n",
       " '피해자는': 345,\n",
       " '알바': 346,\n",
       " '박': 347,\n",
       " '대통령이': 348,\n",
       " '자기': 349,\n",
       " '있을': 350,\n",
       " '말을': 351,\n",
       " '보는': 352,\n",
       " '남': 353,\n",
       " '겁니다': 354,\n",
       " '없이': 355,\n",
       " '대가': 356,\n",
       " '관련': 357,\n",
       " '고충에': 358,\n",
       " '무인': 359,\n",
       " '편의점': 360,\n",
       " '악의적': 361,\n",
       " '텐데': 362,\n",
       " '말도': 363,\n",
       " '이겨': 364,\n",
       " '그래서': 365,\n",
       " '논평': 366,\n",
       " '도쿄': 367,\n",
       " '다른': 368,\n",
       " '회동': 369,\n",
       " '못하면': 370,\n",
       " '이젠': 371,\n",
       " '난': 372,\n",
       " '광진을': 373,\n",
       " '긴': 374,\n",
       " '대진은': 375,\n",
       " '측이': 376,\n",
       " '그럼': 377,\n",
       " '박근혜': 378,\n",
       " '운동': 379,\n",
       " '많은': 380,\n",
       " '서울에': 381,\n",
       " '국민들': 382,\n",
       " '문재인의': 383,\n",
       " '으로': 384,\n",
       " '시민참여유세': 385,\n",
       " '사업가의': 386,\n",
       " '처절한': 387,\n",
       " '외침': 388,\n",
       " '정치판을': 389,\n",
       " '가해한': 390,\n",
       " '으르렁대고': 391,\n",
       " '숨죽이는': 392,\n",
       " '경선서': 393,\n",
       " '김진애를': 394,\n",
       " '정권을': 395,\n",
       " '계속': 396,\n",
       " '청와대': 397,\n",
       " '선택': 398,\n",
       " '볼': 399,\n",
       " '알고': 400,\n",
       " '도': 401,\n",
       " '지지': 402,\n",
       " '무조건': 403,\n",
       " '놈이': 404,\n",
       " '없고': 405,\n",
       " '놈들이': 406,\n",
       " '정권의': 407,\n",
       " '시간': 408,\n",
       " '여사님': 409,\n",
       " '이번에': 410,\n",
       " '보수': 411,\n",
       " '역시': 412,\n",
       " '오늘': 413,\n",
       " '구속된': 414,\n",
       " '빨리': 415,\n",
       " '그게': 416,\n",
       " '확정': 417,\n",
       " '어느': 418,\n",
       " '지': 419,\n",
       " '시민이': 420,\n",
       " '오세훈에': 421,\n",
       " '라떼레전드': 422,\n",
       " '감동실화': 423,\n",
       " '퉤': 424,\n",
       " '허경영': 425,\n",
       " '운동했다': 426,\n",
       " '우위': 427,\n",
       " '종북': 428,\n",
       " '국민들이': 429,\n",
       " '세': 430,\n",
       " '더러운': 431,\n",
       " '분': 432,\n",
       " '철수': 433,\n",
       " '항상': 434,\n",
       " '가장': 435,\n",
       " '부산시민에게': 436,\n",
       " '정치판': 437,\n",
       " 'ㅋㅋ': 438,\n",
       " '모르는': 439,\n",
       " '선두': 440,\n",
       " '선거에': 441,\n",
       " '훈수': 442,\n",
       " '뿌리': 443,\n",
       " '김종철': 444,\n",
       " '총질': 445,\n",
       " '윤석열은': 446,\n",
       " '정치적': 447,\n",
       " '짓': 448,\n",
       " '피해': 449,\n",
       " '박영선에': 450,\n",
       " '차라리': 451,\n",
       " '윤희숙': 452,\n",
       " '정의당': 453,\n",
       " '민주당이': 454,\n",
       " '되고': 455,\n",
       " '그런데': 456,\n",
       " '있고': 457,\n",
       " '하게': 458,\n",
       " '홍준표': 459,\n",
       " '있다는': 460,\n",
       " '국회의원': 461,\n",
       " '호소인': 462,\n",
       " '그래도': 463,\n",
       " '싶다': 464,\n",
       " '안철수에': 465,\n",
       " '한다고': 466,\n",
       " '국민은': 467,\n",
       " '이언주': 468,\n",
       " '의원님': 469,\n",
       " '광고': 470,\n",
       " '출시': 471,\n",
       " '받았으면서': 472,\n",
       " '위축될': 473,\n",
       " '진화된': 474,\n",
       " '흔들': 475,\n",
       " '심야': 476,\n",
       " '전에는': 477,\n",
       " '끝내자': 478,\n",
       " '박영선은': 479,\n",
       " '것입니다': 480,\n",
       " '내부': 481,\n",
       " '만든': 482,\n",
       " '점점': 483,\n",
       " '여자': 484,\n",
       " '못한': 485,\n",
       " '미친': 486,\n",
       " '사람을': 487,\n",
       " '때문이다': 488,\n",
       " '토착': 489,\n",
       " '승리하는': 490,\n",
       " '속도': 491,\n",
       " '흘려': 492,\n",
       " '키즈': 493,\n",
       " '정옥임': 494,\n",
       " '함께': 495,\n",
       " '후보는': 496,\n",
       " '소리': 497,\n",
       " '어디': 498,\n",
       " '들': 499,\n",
       " '표를': 500,\n",
       " '네가': 501,\n",
       " '거야': 502,\n",
       " '문재인은': 503,\n",
       " '당이': 504,\n",
       " '근데': 505,\n",
       " '아니': 506,\n",
       " '올린': 507,\n",
       " '날입니다': 508,\n",
       " '켜야': 509,\n",
       " '불렀다': 510,\n",
       " '조화': 511,\n",
       " '언어': 512,\n",
       " '않을': 513,\n",
       " '여': 514,\n",
       " '본다': 515,\n",
       " '기': 516,\n",
       " '땀': 517,\n",
       " '봉사한': 518,\n",
       " '특혜라고': 519,\n",
       " '하십니까': 520,\n",
       " '오세훈박영선': 521,\n",
       " '지기도': 522,\n",
       " '붙은': 523,\n",
       " '겨냥': 524,\n",
       " '깡패의': 525,\n",
       " '있으면서': 526,\n",
       " '먼저': 527,\n",
       " '본': 528,\n",
       " '시': 529,\n",
       " '건가': 530,\n",
       " '이명박': 531,\n",
       " '가지고': 532,\n",
       " '놈': 533,\n",
       " '끝까지': 534,\n",
       " '됩니다': 535,\n",
       " '남편': 536,\n",
       " '거냐': 537,\n",
       " '앞으로': 538,\n",
       " '언급은': 539,\n",
       " '트위터': 540,\n",
       " '엘시티로': 541,\n",
       " '대박': 542,\n",
       " '한국': 543,\n",
       " '다음': 544,\n",
       " '야': 545,\n",
       " '가는': 546,\n",
       " '위한': 547,\n",
       " '전에': 548,\n",
       " '자가': 549,\n",
       " '후보로': 550,\n",
       " '사람들이': 551,\n",
       " '때까지': 552,\n",
       " 'ㅋㅋㅋ': 553,\n",
       " '가진': 554,\n",
       " '단일화를': 555,\n",
       " '해서': 556,\n",
       " '요': 557,\n",
       " '지지율이': 558,\n",
       " '자신의': 559,\n",
       " '뭘': 560,\n",
       " '뿐이다': 561,\n",
       " '일을': 562,\n",
       " '같이': 563,\n",
       " '사는': 564,\n",
       " '어': 565,\n",
       " '팔아': 566,\n",
       " 'ㅋ': 567,\n",
       " '성추행이': 568,\n",
       " '보인다': 569,\n",
       " '북한': 570,\n",
       " '사랑합니다': 571,\n",
       " '차례': 572,\n",
       " '김종인의': 573,\n",
       " '조민': 574,\n",
       " '든': 575,\n",
       " '여러분': 576,\n",
       " '이길': 577,\n",
       " '안철수의': 578,\n",
       " '검찰': 579,\n",
       " '있으니': 580,\n",
       " '부른': 581,\n",
       " '대선': 582,\n",
       " '서로': 583,\n",
       " '안철수와': 584,\n",
       " '수가': 585,\n",
       " '한다는': 586,\n",
       " '보다': 587,\n",
       " '노무현': 588,\n",
       " '한심한': 589,\n",
       " '정치를': 590,\n",
       " '너희': 591,\n",
       " '바랍니다': 592,\n",
       " '당연히': 593,\n",
       " '부산대': 594,\n",
       " '힘을': 595,\n",
       " '일단': 596,\n",
       " '보기': 597,\n",
       " '온': 598,\n",
       " '쓰레기': 599,\n",
       " '뭐가': 600,\n",
       " '억': 601,\n",
       " '되지': 602,\n",
       " '정의': 603,\n",
       " '변창흠': 604,\n",
       " '오': 605,\n",
       " '대한민국의': 606,\n",
       " '영입': 607,\n",
       " '빈손': 608,\n",
       " '이제는': 609,\n",
       " '윤석열': 610,\n",
       " '사퇴': 611,\n",
       " '것들': 612,\n",
       " '씨가': 613,\n",
       " '너희들이': 614,\n",
       " '차': 615,\n",
       " '당원': 616,\n",
       " '인간이': 617,\n",
       " '크게': 618,\n",
       " '조사': 619,\n",
       " '힘은': 620,\n",
       " '건강하세요': 621,\n",
       " '정권이': 622,\n",
       " '다인지': 623,\n",
       " '날': 624,\n",
       " '그만': 625,\n",
       " '있는데': 626,\n",
       " '와서': 627,\n",
       " '때는': 628,\n",
       " '하지만': 629,\n",
       " '열심히': 630,\n",
       " '우파': 631,\n",
       " '되어': 632,\n",
       " '대한민국을': 633,\n",
       " '침대': 634,\n",
       " '축구냐': 635,\n",
       " '초조하냐': 636,\n",
       " '거칠어진다': 637,\n",
       " '경선': 638,\n",
       " '그러니': 639,\n",
       " '즉': 640,\n",
       " '뭔': 641,\n",
       " '먹고': 642,\n",
       " '크크크': 643,\n",
       " '너': 644,\n",
       " '감사합니다': 645,\n",
       " '팔려': 646,\n",
       " '못하겠다': 647,\n",
       " '쇼크': 648,\n",
       " '뒤늦게': 649,\n",
       " '못할': 650,\n",
       " '같은데': 651,\n",
       " '그러나': 652,\n",
       " '위': 653,\n",
       " '대해': 654,\n",
       " '가서': 655,\n",
       " '하는데': 656,\n",
       " '하면서': 657,\n",
       " '말하는': 658,\n",
       " '결과': 659,\n",
       " '봐라': 660,\n",
       " '참으로': 661,\n",
       " '않은': 662,\n",
       " '씨는': 663,\n",
       " '아예': 664,\n",
       " '사전투표': 665,\n",
       " '조사하라': 666,\n",
       " '놓고': 667,\n",
       " '것인가': 668,\n",
       " '난다': 669,\n",
       " '있어': 670,\n",
       " '우리나라': 671,\n",
       " '대로': 672,\n",
       " '이번에도': 673,\n",
       " '전혀': 674,\n",
       " '스스로': 675,\n",
       " '발': 676,\n",
       " '조': 677,\n",
       " '들고': 678,\n",
       " '생각': 679,\n",
       " '늘': 680,\n",
       " '유은혜': 681,\n",
       " '정권은': 682,\n",
       " '두고': 683,\n",
       " '힘내세요': 684,\n",
       " '들어': 685,\n",
       " '대표': 686,\n",
       " '나라의': 687,\n",
       " '당시': 688,\n",
       " '세금': 689,\n",
       " '하니': 690,\n",
       " '아는': 691,\n",
       " '봐': 692,\n",
       " '물론': 693,\n",
       " '인간': 694,\n",
       " '따라': 695,\n",
       " '아무리': 696,\n",
       " '그동안': 697,\n",
       " '어차피': 698,\n",
       " '언론': 699,\n",
       " '국가': 700,\n",
       " '이거': 701,\n",
       " '있네': 702,\n",
       " '몇': 703,\n",
       " '탓': 704,\n",
       " '너희들': 705,\n",
       " '거로': 706,\n",
       " '야당이': 707,\n",
       " '통': 708,\n",
       " '윤': 709,\n",
       " '중도': 710,\n",
       " '판세는': 711,\n",
       " '반대': 712,\n",
       " '딜레마': 713,\n",
       " '범위': 714,\n",
       " '효과': 715,\n",
       " '김정길': 716,\n",
       " '미국': 717,\n",
       " '이번에는': 718,\n",
       " '민주당은': 719,\n",
       " '제일': 720,\n",
       " '서울시': 721,\n",
       " '앞선': 722,\n",
       " '것으로': 723,\n",
       " '이건': 724,\n",
       " '선거에서': 725,\n",
       " '그러면': 726,\n",
       " '지지율': 727,\n",
       " '명': 728,\n",
       " '현재': 729,\n",
       " '국회': 730,\n",
       " '생각이': 731,\n",
       " '편': 732,\n",
       " '지지합니다': 733,\n",
       " '지난달': 734,\n",
       " '한일': 735,\n",
       " '흑석': 736,\n",
       " '근무': 737,\n",
       " '오차': 738,\n",
       " '바꾸자니': 739,\n",
       " '레임덕이': 740,\n",
       " '놔두자니': 741,\n",
       " '재보선이': 742,\n",
       " '현': 743,\n",
       " '봐도': 744,\n",
       " '가짜': 745,\n",
       " '는': 746,\n",
       " '의원': 747,\n",
       " '적폐': 748,\n",
       " '당선': 749,\n",
       " '있다고': 750,\n",
       " '너는': 751,\n",
       " '살': 752,\n",
       " '갈': 753,\n",
       " '입당하라': 754,\n",
       " '들어오면': 755,\n",
       " '에': 756,\n",
       " '시너지': 757,\n",
       " '샀던': 758,\n",
       " '입': 759,\n",
       " '아직': 760,\n",
       " '선거는': 761,\n",
       " '내년': 762,\n",
       " '살고': 763,\n",
       " '힘이': 764,\n",
       " '있어야': 765,\n",
       " '것들이': 766,\n",
       " '꼴': 767,\n",
       " '말은': 768,\n",
       " '철저히': 769,\n",
       " '아니냐': 770,\n",
       " '없다고': 771,\n",
       " '젊은': 772,\n",
       " '사태': 773,\n",
       " '차기': 774,\n",
       " '김종인은': 775,\n",
       " '위해서': 776,\n",
       " '법을': 777,\n",
       " '이상': 778,\n",
       " '했다': 779,\n",
       " '미': 780,\n",
       " '문재인과': 781,\n",
       " '태극기': 782,\n",
       " '정책': 783,\n",
       " '입당해야': 784,\n",
       " '재보궐': 785,\n",
       " '없다는': 786,\n",
       " '만들어': 787,\n",
       " '박영선을': 788,\n",
       " '선거를': 789,\n",
       " '받는': 790,\n",
       " '말인가': 791,\n",
       " '남이': 792,\n",
       " '똥': 793,\n",
       " '코로나': 794,\n",
       " '여성': 795,\n",
       " '기가': 796,\n",
       " '딱': 797,\n",
       " '앞에': 798,\n",
       " '권력을': 799,\n",
       " '앵커': 800,\n",
       " '지지하는': 801,\n",
       " '아들': 802,\n",
       " '그걸': 803,\n",
       " '건데': 804,\n",
       " '행위': 805,\n",
       " '미국은': 806,\n",
       " '박영선오세훈': 807,\n",
       " '선생': 808,\n",
       " '철수는': 809,\n",
       " '극대화': 810,\n",
       " '문가': 811,\n",
       " '선': 812,\n",
       " '이낙연': 813,\n",
       " '된다고': 814,\n",
       " '않으면': 815,\n",
       " '출신': 816,\n",
       " '하자': 817,\n",
       " '하겠다고': 818,\n",
       " '없나': 819,\n",
       " '인제': 820,\n",
       " '더듬어': 821,\n",
       " '서울시장은': 822,\n",
       " '정치는': 823,\n",
       " '완전히': 824,\n",
       " '특히': 825,\n",
       " '민주당의': 826,\n",
       " '오히려': 827,\n",
       " '세상이': 828,\n",
       " '대한민국이': 829,\n",
       " '않나': 830,\n",
       " '능력': 831,\n",
       " '타고': 832,\n",
       " '있을까': 833,\n",
       " '너희들은': 834,\n",
       " '주는': 835,\n",
       " '원래': 836,\n",
       " '아무': 837,\n",
       " '없습니다': 838,\n",
       " '나올': 839,\n",
       " '말아': 840,\n",
       " '않는다': 841,\n",
       " '부정선거': 842,\n",
       " '해놓고': 843,\n",
       " '검토': 844,\n",
       " '투표합시다': 845,\n",
       " '내곡': 846,\n",
       " '해저터널': 847,\n",
       " '구': 848,\n",
       " '시민': 849,\n",
       " '대권': 850,\n",
       " '되든': 851,\n",
       " '지금까지': 852,\n",
       " '인간들': 853,\n",
       " '사람들': 854,\n",
       " '김': 855,\n",
       " '가지': 856,\n",
       " '자리': 857,\n",
       " '나와서': 858,\n",
       " '배': 859,\n",
       " '잘못': 860,\n",
       " '전부': 861,\n",
       " '그리': 862,\n",
       " '인물이': 863,\n",
       " '범법': 864,\n",
       " '언제': 865,\n",
       " '백신': 866,\n",
       " '여러': 867,\n",
       " '알': 868,\n",
       " '변절자': 869,\n",
       " '정': 870,\n",
       " '기호': 871,\n",
       " '현장': 872,\n",
       " '뉴있저': 873,\n",
       " '대전보궐선거': 874,\n",
       " '판세': 875,\n",
       " '영향은': 876,\n",
       " '길': 877,\n",
       " '성': 878,\n",
       " '박원순이': 879,\n",
       " '자유': 880,\n",
       " '박형준이': 881,\n",
       " '봅니다': 882,\n",
       " '시민들': 883,\n",
       " '추행': 884,\n",
       " '조금': 885,\n",
       " '짓을': 886,\n",
       " '있으면': 887,\n",
       " '있지': 888,\n",
       " '뭔가': 889,\n",
       " '없는데': 890,\n",
       " '더불어민주당의': 891,\n",
       " '여당': 892,\n",
       " '국민에게': 893,\n",
       " '정당': 894,\n",
       " '누구': 895,\n",
       " '물타기': 896,\n",
       " '문재인을': 897,\n",
       " '벌써': 898,\n",
       " '조작': 899,\n",
       " '방송': 900,\n",
       " '반성하며': 901,\n",
       " '영입한': 902,\n",
       " '김문수에': 903,\n",
       " '질문했다': 904,\n",
       " '잘려': 905,\n",
       " '마': 906,\n",
       " '숨은': 907,\n",
       " '갑니다': 908,\n",
       " 'ㅠㅠ': 909,\n",
       " '예': 910,\n",
       " '저도': 911,\n",
       " '왜구': 912,\n",
       " '달랜': 913,\n",
       " '없을': 914,\n",
       " '썩은': 915,\n",
       " '시장이': 916,\n",
       " '대한민국은': 917,\n",
       " '약발': 918,\n",
       " '먹히나': 919,\n",
       " '독주': 920,\n",
       " '진정한': 921,\n",
       " '법': 922,\n",
       " '자기들': 923,\n",
       " '전라도': 924,\n",
       " '맞는': 925,\n",
       " '때가': 926,\n",
       " '삶은': 927,\n",
       " '하하': 928,\n",
       " '영선': 929,\n",
       " '말아야': 930,\n",
       " '돼': 931,\n",
       " '말이야': 932,\n",
       " '달라': 933,\n",
       " '정도': 934,\n",
       " '가능성이': 935,\n",
       " '생각합니다': 936,\n",
       " '로맨스': 937,\n",
       " '반값': 938,\n",
       " '건지': 939,\n",
       " '아냐': 940,\n",
       " '거고': 941,\n",
       " '진': 942,\n",
       " '번도': 943,\n",
       " '준': 944,\n",
       " '안철수를': 945,\n",
       " '조강지처': 946,\n",
       " '버렸다에': 947,\n",
       " '묵과': 948,\n",
       " '손은': 949,\n",
       " '난타전내곡동': 950,\n",
       " '몹쓸짓': 951,\n",
       " '말이다': 952,\n",
       " '만난': 953,\n",
       " '바뀌면': 954,\n",
       " '교회': 955,\n",
       " '달': 956,\n",
       " '나와': 957,\n",
       " '눈': 958,\n",
       " '철수야': 959,\n",
       " '사건': 960,\n",
       " '라': 961,\n",
       " '싶은': 962,\n",
       " '간다': 963,\n",
       " '좋겠다': 964,\n",
       " '남의': 965,\n",
       " '똑같은': 966,\n",
       " '과거': 967,\n",
       " '나도': 968,\n",
       " '속': 969,\n",
       " '내고': 970,\n",
       " '눈물': 971,\n",
       " '문제가': 972,\n",
       " '돈으로': 973,\n",
       " '나온다': 974,\n",
       " '올': 975,\n",
       " '받고': 976,\n",
       " '자체가': 977,\n",
       " '하기': 978,\n",
       " '평당': 979,\n",
       " '원대': 980,\n",
       " '실현': 981,\n",
       " '년대': 982,\n",
       " '로': 983,\n",
       " '없으니': 984,\n",
       " '주': 985,\n",
       " '말라': 986,\n",
       " '국민들의': 987,\n",
       " '서': 988,\n",
       " '저격': 989,\n",
       " '뻔뻔': 990,\n",
       " '어려운': 991,\n",
       " '받아': 992,\n",
       " '인터뷰': 993,\n",
       " '모습': 994,\n",
       " '쓰는': 995,\n",
       " '진성준': 996,\n",
       " '불법': 997,\n",
       " '당신이': 998,\n",
       " '초조한': 999,\n",
       " '배수진': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도가 높은 단어들로 구성\n",
    "num_word = 5000\n",
    "\n",
    "token = Tokenizer(num_words=num_word) # 토큰화 함수 지정\n",
    "token.fit_on_texts(docs) # 토큰화 함수에 문장 적용\n",
    "token.word_index # 각 단어에 매겨진 인덱스값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:11.961660Z",
     "start_time": "2021-05-11T15:52:11.956693Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30503"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token.word_index) # 전체 토큰 개수는 30503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:14.085306Z",
     "start_time": "2021-05-11T15:52:13.994395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[785, 68, 70, 36, 1659, 1409, 1006, 479, 3316, 1221, 21, 1660, 682, 877, 59]\n",
      "재보궐  부산 김영춘  박형준 서울도 양자 대결 박영선은 정동영이 얻은  프로 선에 머무를 것  이후  정권은 몰락의 길 페달을 급속히 밟을 것이다\n"
     ]
    }
   ],
   "source": [
    "# 앞서 만든 토큰의 인덱스로만 채워진 새로운 배열 생성\n",
    "X = token.texts_to_sequences(docs)\n",
    "print(X[0])  # 토큰화 후 인덱스로 채워진 새로운 배열\n",
    "print(docs[0])  # 토큰화 전 실제 문장의 배열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤗 문장별 토큰 개수 파악\n",
    "- 최대 토큰수를 찾기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:16.141766Z",
     "start_time": "2021-05-11T15:52:16.136780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 토큰으로 채워진 배열에서 최대 토큰수 찾기\n",
    "max_len = 0\n",
    "for i in X:\n",
    "    if max_len <= len(i):\n",
    "        max_len = len(i)\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:18.634298Z",
     "start_time": "2021-05-11T15:52:18.203471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAEuCAYAAACnC+ctAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAcpUlEQVR4nO3dbWxUVeLH8V9b+4COtB007EKk+IJ1mukGsjaYLGkrZYHGVcKYNK0sthtSCAmoaCOQjatulqDZaAurQ6Qoa9XKU6hhU2iTXREF48alZinSVqxkDcaYdpDpg90yMHP/Lwz3zwilQznT24fv51U5Zx7OPRfJ1zvTmQTLsiwBAADgpiQ6vQAAAIDxgKgCAAAwgKgCAAAwgKgCAAAwgKgCAAAwgKgCAAAw4BanF9Dc3Oz0EgAAAGJ27733XnPc8aiSBl/czWpra1N2dnZcHhtDY/+dxzlwFvvvPM6Bs8bj/l/vYhAv/wEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVEmaufGgZm486PQyAADAGEZUAQAAGEBUAQAAGEBUAQAAGEBUAQAAGDBkVG3fvl1z585VYWGhCgsL9eijj9pz1dXVKiwsVEFBgWpqaqLu19jYqKKiIuXl5amyslIDAwPmVw8AADBKDBlVPT09euyxx3T48GEdPnxYb7/9tiRp7969am9vV1NTkw4cOKD6+nodO3ZMktTR0aGqqirV1tbqyJEjsixL27Zti++RAAAAOCimqLr99tuvGt+9e7fWrFmjlJQUZWRkaPny5WpoaJAk7d+/XyUlJZo6daqSkpK0evVqew4AAGA8GjKquru7NXny5KixixcvqqOjQ16v1x7zer06ffq0JOnkyZOaPXu2PTdr1iwFAgH19fWZWjcAAMCocstQN+jt7dXzzz+vTZs2KScnR08//bRSU1PlcrmUlJRk387tdisYDEqSurq65Ha77bmEhARlZGSou7tbLpfrqudoa2szcSxXGRgYuKHHjtc6Jqob3X+YxzlwFvvvPM6Bsyba/g8ZVW+88YYSExN14cIFvfXWW1q5cqX+9re/ybKsqNuFw2ElJv544SsSiVw1H4lE7Pmfys7OHu76r6utrS3Gxz4T13VMVLHvP+KFc+As9t95nANnjcf9b25uHnRuyJf/LodQamqqVq5cqcTERH377bfq7e2NCqdgMKgpU6ZIktLT0+2rVpf19PQoMzNzWAcAAAAw2t3w51SFw2Glp6dr2rRpOnXqlD3e0tKinJwcSZLH41FLS4s919raqqysLKWlpRlYMgAAwOgzZFR98sknsixLlmWptrZWaWlpuvvuu+Xz+eT3+xUKhRQIBLRr1y4VFxdLkpYuXaq6ujp1dnYqFArJ7/ertLQ07gcDAADglCGjaseOHZo3b54WLFig5uZmvfbaa0pKSlJFRYUyMzOVn5+vkpISrV27Vh6PR5KUm5ursrIy+Xw+LViwQFlZWVq2bFncDwYAAMApQ75RfefOndccT05O1ubNmwe9X3l5ucrLy4e/MgAAgDGE7/4DAAAwgKgCAAAwgKgCAAAwgKgCAAAwgKgCAAAwgKgCAAAwgKgCAAAwgKgCAAAwgKgCAAAwgKgCAAAwgKi6wsyNBzVz40GnlwEAAMYgogoAAMAAogoAAMAAogoAAMAAogoAAMAAogoAAMAAogoAAMAAogoAAMAAogoAAMAAogoAAMAAogoAAMAAogoAAMAAomoQfA8gAAC4EUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAAUQVAACAATcUVT6fT9XV1fafq6urVVhYqIKCAtXU1ETdtrGxUUVFRcrLy1NlZaUGBgbMrBgAAGAUijmqjh07pi+++ML+8969e9Xe3q6mpiYdOHBA9fX1OnbsmCSpo6NDVVVVqq2t1ZEjR2RZlrZt22Z+9QAAAKNETFEViUS0detWPfjgg/bY7t27tWbNGqWkpCgjI0PLly9XQ0ODJGn//v0qKSnR1KlTlZSUpNWrV9tzAAAA41FMUbV3717NmTNHd911lyTp4sWL6ujokNfrtW/j9Xp1+vRpSdLJkyc1e/Zse27WrFkKBALq6+szuXYAAIBR45ahbnD27Fnt3LlT+/bt01tvvSVJOn/+vFwul5KSkuzbud1uBYNBSVJXV5fcbrc9l5CQoIyMDHV3d8vlcl31HG1tbTd9INcyMDAwrMe+8j7xWttEMNz9hzmcA2ex/87jHDhrou3/daMqEolow4YNeuqpp5Senh41bllW1G3D4bASExMHnY9EIvb8T2VnZw9r8UNpa2uL8bHPXGM9Z674GcMR+/4jXjgHzmL/ncc5cNZ43P/m5uZB56778t/rr7+uO++8U0VFRVHjkydPVm9vb1Q4BYNBTZkyRZKUnp5uX7W6rKenR5mZmTe8eAAAgLHguleq6uvr1dnZqdzcXEnShQsXJEnt7e2aNm2aTp06pZycHElSS0uL/bPH41FLS4t9v9bWVmVlZSktLS1uBwIAAOCk616pampq0meffabjx4/r+PHjWrVqlVasWKHt27fL5/PJ7/crFAopEAho165dKi4uliQtXbpUdXV16uzsVCgUkt/vV2lp6YgcEAAAgBOG/YnqFRUVyszMVH5+vkpKSrR27Vp5PB5JUm5ursrKyuTz+bRgwQJlZWVp2bJlxhYNAAAw2gz5239Xeuyxx+yfk5OTtXnz5kFvW15ervLy8uGvDAAAYAzhu/8AAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMIKoAAAAMiCmqampqtGjRIhUUFKisrExff/21PVddXa3CwkIVFBSopqYm6n6NjY0qKipSXl6eKisrNTAwYHb1AAAAo0RMUfWrX/1KjY2N+vDDD5WXl6fnn39ekrR37161t7erqalJBw4cUH19vY4dOyZJ6ujoUFVVlWpra3XkyBFZlqVt27bF7UAAAACcFFNU5ebmKikpSZKUn5+vzs5OSdLu3bu1Zs0apaSkKCMjQ8uXL1dDQ4Mkaf/+/SopKdHUqVOVlJSk1atX23NjzcyNBzVz40GnlwEAAEaxG3pPVXd3t958802Vlpbq4sWL6ujokNfrtee9Xq9Onz4tSTp58qRmz55tz82aNUuBQEB9fX2Glg4AADB63BLLjb744gtVVFSos7NTDz74oIqLi3X+/Hm5XC77CpYkud1uBYNBSVJXV5fcbrc9l5CQoIyMDHV3d8vlckU9fltbm4ljucrAwMCwHvvK+wz2M4Y23P2HOZwDZ7H/zuMcOGui7X9MUXXPPffo6NGj6uvr0+uvv64VK1bo5ZdflmVZUbcLh8NKTPzx4lckErlqPhKJ2PNXys7OHu76r6utrS3Gxz5zjfWcGeRnxCr2/Ue8cA6cxf47j3PgrPG4/83NzYPO3dDLfy6XS+vWrdN3332n7u5u9fb2RoVTMBjUlClTJEnp6en2VavLenp6lJmZeSNPCQAAMCYM63OqkpOTNWnSJE2bNk2nTp2yx1taWpSTkyNJ8ng8amlpsedaW1uVlZWltLS0m1wyAADA6DNkVJ07d06HDh1SOByWJNXV1SkzM1MzZsyQz+eT3+9XKBRSIBDQrl27VFxcLElaunSp6urq1NnZqVAoJL/fr9LS0vgeDQAAgEOGjKrk5GTt2bNHeXl5Wrhwof7zn//o1VdfVUJCgioqKpSZman8/HyVlJRo7dq18ng8kn78GIaysjL5fD4tWLBAWVlZWrZsWdwPCAAAwAlDvlF98uTJqq2tveZccnKyNm/ePOh9y8vLVV5ePvzVAQAAjBF89x8AAIABRBUAAIABRBUAAIABRBUAAIABRBUAAIABRBUAAIABRBUAAIABRBUAAIABRBUAAIABRBUAAIABRBUAAIABRNU1zNx40OklAACAMYaoAgAAMICoAgAAMICoAgAAMICoAgAAMICoAgAAMICougH8ViAAABgMUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGDAhI2qmRsPaubGg04vAwAAjBMTNqoAAABMIqoAAAAMIKoAAAAMIKoAAAAMiCmqGhsb5fP5NH/+fJWUlKi9vd2eq66uVmFhoQoKClRTU3PV/YqKipSXl6fKykoNDAyYXT0AAMAoEVNUffTRR9qxY4c++OADPfLII3riiSckSXv37lV7e7uampp04MAB1dfX69ixY5Kkjo4OVVVVqba2VkeOHJFlWdq2bVv8jgQAAMBBMUXVCy+8oDvuuEOStHTpUgWDQQUCAe3evVtr1qxRSkqKMjIytHz5cjU0NEiS9u/fr5KSEk2dOlVJSUlavXq1PTeW8LELAAAgFjf8nqqBgQFduHBBaWlp6ujokNfrtee8Xq9Onz4tSTp58qRmz55tz82aNUuBQEB9fX0Glg0AADC63HKjd9i+fbvmzZun/v5+uVwuJSUl2XNut1vBYFCS1NXVJbfbbc8lJCQoIyND3d3dcrlcUY/Z1tY23PVf18DAwJCPfaPPHa+1jkex7D/ii3PgLPbfeZwDZ020/Y85qiKRiLZs2aIjR47ozTffVCgUkmVZUbcJh8NKTEy0b//T+UgkYs9fKTs7ezhrH1JbW9t1HvvMFc99JubHjNdax6Pr7z9GAufAWey/8zgHzhqP+9/c3DzoXExR1d/fr8cff1wpKSl699135XK51N/fr97eXlmWpYSEBElSMBjUlClTJEnp6en2VavLenp6lJmZOdzjiAveMwUAAEyI6T1V69ev1/Tp0+X3++2X7m699VZNmzZNp06dsm/X0tKinJwcSZLH41FLS4s919raqqysLKWlpZlcPwAAwKgwZFR1dnbq008/1TPPPGNfkbrM5/PJ7/crFAopEAho165dKi4ulvTjbwnW1dWps7NToVBIfr9fpaWl8TkKAAAAhw358t/Zs2fV39+vxYsXR42vX79eFRUVeu6555Sfn6/bbrtN69atk8fjkSTl5uaqrKxMPp9PiYmJeuihh7Rs2bL4HAUAAIDDhoyqe++9V59//vmg85s3bx50rry8XOXl5cNbGQAAwBjCd/8BAAAYQFQBAAAYQFQBAAAYQFQBAAAYQFQBAAAYQFQBAAAYQFQBAAAYQFQNE98ZCAAArkRUAQAAGEBUAQAAGEBUAQAAGEBUAQAAGEBUAQAAGEBUAQAAGEBUAQAAGEBUAQAAGEBUAQAAGEBUGcCnqwMAAKIKAADAAKIKAADAAKIKAADAAKIKAADAAKIKAADAAKIKAADAAKIKAADAAKIKAADAAKIKAADAAKIKAADAAKIKAADAAKLKkJkbD/IdgAAATGBEFQAAgAFEFQAAgAFEFQAAgAFEFQAAgAG3OL2AsYY3owMAgGvhShUAAIABRBUAAIABRBUAAIABRBUAAIABMUdVKBTSSy+9pK1bt0aNPfvss5o/f74KCwtVX18fdZ933nlHCxcuVF5enjZt2qRIJGJu5QAAAKNITFH197//XYsWLdKhQ4eiwsjv90uS3n//fb399tt6+eWX9dVXX0mSPv74Y9XX1+u9997TP/7xD7W1tWnfvn1xOAQAAADnxRRVly5d0pYtW+Tz+ewxy7K0b98+PfHEE0pMTNT06dO1ZMkSHTp0SJK0Z88erVy5Ui6XS2lpaaqoqFBDQ0N8jgIAAMBhMUXVww8/rDlz5kSNffPNN5o0aZKmTJlij3m9Xp0+fVqSdPLkSc2ePfuacwAAAOPNsD/8MxAIyO12R4253W4Fg0FJUldXV9S82+1Wd3f3NR+rra1tuMu4roGBgbg9tnTtdcfz+caaeO8/hsY5cBb77zzOgbMm2v4PO6rC4bAsy7pqLDHxx4tfkUgkaj4SiSghIeGaj5WdnT3cZVxXW1vbdR77zE0//v8/9plrjOH6+4+RwDlwFvvvPM6Bs8bj/jc3Nw86N+yPVMjIyLjqylMwGLRfDkxPT4+aP3/+fNRLhQAAAOPJsK9UzZgxQ+fOndP3339vv8x34sQJ5eTkSJI8Ho9OnDihn/3sZ5KklpYWe2684HsAAQDAZcO+UpWSkqIHHnhAr7zyisLhsM6cOaPDhw9ryZIlkiSfz6eamhr19fXphx9+0I4dO/TII48YWzgAAMBoMuwrVZK0YcMGbdiwQfPmzVNmZqY2bdqkO+64Q5K0ZMkSffnll1q4cKFSU1NVXl6ugoICI4se7S5fwfrvi791eCUAAGCk3FBUPfbYY1F/vv3227Vt27ZBb19ZWanKysrhrQwAAGAM4bv/AAAADCCqAAAADCCqAAAADCCqAAAADCCqAAAADCCqAAAADCCqAAAADCCqAAAADCCqRgDfEQgAwPhHVAEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVAEAABhAVI2QmRsP8nlVAACMY0QVAACAAUSVYVyNAgBgYrrF6QWMZwQWAAATB1eqAAAADJhQUTVa3iw+WtYBAADMmVBRBQAAEC9EFQAAgAFEFQAAgAETMqp4PxMAADBtQkYVAACAaUQVAACAAXz4p8OufCnyvy/+1sGVAACAm8GVKgAAAAOIqhHGm+QBABifiKpR6MrwIsIAABgbiCoAAAADiCoAAAADiCoH8dIeAADjB1E1BhFjAACMPkTVKDVz40E7nq78+Vq3AwAAziOqAAAADCCqAAAADIjr19T09PTomWeeUUtLi1JTU/WHP/xBBQUF8XzKQY2Fl8luZI3xOp6ZGw/ydTkAAAxDXKPqT3/6k7Kzs/XXv/5Vp06d0ooVK9TU1KTMzMx4Pu24Fcv7qgYLomvd5vLYf1/87ZiITgAARrO4RVVPT48+/vhjvfjii5Ikr9er++67Tx988IEefvjheD3thBdLHA11mytj68qxWK9gRQdcdkz3AQBgrItbVLW2tuoXv/iFkpOT7TGv16vTp0/H6ylh2E/ja7AY++mVr+s9Hi8tAgDGqwTLsqx4PHBDQ4P++c9/asuWLfbYvn379Nlnn+mFF16wx5qbm+Px9AAAAHFx7733XnM8bleqwuGwftpr4XBYCQkJMS0MAABgLInbRypkZGSou7s7aiwYDGrKlCnxekoAAADHxC2qPB6PWltbdenSJXvsxIkTysnJiddTAgAAOCZuUTV16lR5vV7t3LlTlmXp+PHj+uqrr3T//ffH6yltPT09evzxx3X//fdr8eLF+vDDD+P+nBNRKBTSSy+9pK1bt0aNPfvss5o/f74KCwtVX18fdZ933nlHCxcuVF5enjZt2qRIJDLSyx43Ghsb5fP5NH/+fJWUlKi9vd2eq66uVmFhoQoKClRTU3PV/YqKipSXl6fKykoNDAyM9NLHhZqaGi1atEgFBQUqKyvT119/bc+x/yPL5/Opurra/jP7H3/bt2/X3LlzVVhYqMLCQj366KP23ITefyuOvv32W+t3v/uddd9991k+n89qbW2N59PZnnrqKWvbtm2WZVnW559/bs2dO9f6/vvvR+S5J4oDBw5YBQUF1vz5862qqip7vKqqyvrjH/9ohcNh65tvvrF+/etfWx0dHZZlWdaxY8csn89n9fb2Wv/73/+sZcuWWbt373bqEMa8jRs3Wl1dXZZlWdZ7771nLVq0yLIsy9qzZ4+1atUq68KFC9b58+etxYsXW0ePHrUsy7K+/PJL6ze/+Y313XffWZcuXbKefPJJ6+WXX3bsGMayf//739alS5csy7Ksmpoa6/e//71lWez/SDt69KiVnZ1t/zvE/o+Mv/zlL9Zbb7111fhE3/+4fk3Nz3/+c73zzjv617/+pfr6emVnx/8ziy5/PlZFRYWk6M/HgjmXLl3Sli1b5PP57DHLsrRv3z498cQTSkxM1PTp07VkyRIdOnRIkrRnzx6tXLlSLpdLaWlpqqioUENDg1OHMOa98MILuuOOOyRJS5cuVTAYVCAQ0O7du7VmzRqlpKQoIyNDy5cvt/d5//79Kikp0dSpU5WUlKTVq1dzDoYpNzdXSUlJkqT8/Hx1dnZKEvs/giKRiLZu3aoHH3zQHmP/R0ZPT49uv/32q8Yn+v6Pu+/+4/OxRsbDDz+sOXPmRI198803mjRpUtQvI1y59ydPntTs2bOvOYebMzAwoAsXLigtLU0dHR3yer323PXOwaxZsxQIBNTX1zfiax4vuru79eabb6q0tFQXL15k/0fQ3r17NWfOHN11112SxP6PoO7ubk2ePDlqjP0fh1EVCATkdrujxtxu91W/iQjzBtv7YDAoSerq6oqa57yYs337ds2bN0/9/f1yuVz2FRTp+ucgISHhmr+pi6F98cUXysvL09y5cxUKhVRcXKzz58+z/yPk7Nmz2rlzp9auXWuPsf8jp7e3V88//7wKCwv1+OOP6+zZs+y/xmFUxfr5WDBvsL1PTPzxr1kkEomaj0QinJebFIlEVFVVpffff19//vOfr9pj6frn4PLY5XnE7p577tHRo0fV3Nysu+66SytWrLjh/wYuj7H/NyYSiWjDhg166qmnlJ6eHjXO/o+MN954Qx999JEaGxv1y1/+UitXruTvv8ZhVPH5WM4Zau/T09Oj5s+fP895uQn9/f1atWqVOjo69O6778rtdmvy5Mnq7e2N+ofrp+fg8v81XtbT08OXnN8El8uldevW6bvvvlN3dzf7PwJef/113XnnnSoqKooa5+//yLkcQqmpqVq5cqUSExP17bffTvj9H3dRxedjOWfGjBk6d+6cvv/+e3vsyr33eDw6ceKEPdfS0sJ5uQnr16/X9OnT5ff75XK5JEm33nqrpk2bplOnTtm3u3KfPR6PWlpa7LnW1lZlZWUpLS1tZBc/DiUnJ2vSpEns/wior6/X0aNHlZubq9zcXNXU1Gjnzp168skn2X+HhMNhpaenT/j9H3dR5eTnY010KSkpeuCBB/TKK68oHA7rzJkzOnz4sJYsWSLpx8+SqampUV9fn3744Qft2LFDjzzyiMOrHps6Ozv16aef6plnnrnqJVSfzye/369QKKRAIKBdu3apuLhY0o+/JVhXV6fOzk6FQiH5/X6VlpY6cQhj2rlz53To0CGFw2FJUl1dnTIzMzVjxgz2fwQ0NTXps88+0/Hjx3X8+HGtWrVKK1as0Pbt29n/EfLJJ5/IsixZlqXa2lqlpaXp7rvvnvD7H7fv/nPS5s2b9fTTT2vnzp2aNm2atm7dqtTUVKeXNSFs2LBBGzZs0Lx585SZmalNmzbZv/a/ZMkSffnll1q4cKFSU1NVXl6ugoICh1c8Np09e1b9/f1avHhx1Pj69etVUVGh5557Tvn5+brtttu0bt06eTweST9+DEBZWZl8Pp8SExP10EMPadmyZU4cwpiWnJysPXv2aNOmTbrttts0Z84cvfrqq0pISGD/Hcb+j4wdO3aosrJSaWlpysnJ0WuvvaakpKQJv/8J1k/fNQYAAIAbNu5e/gMAAHACUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGAAUQUAAGDA/wHlPiIChFiqMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 문장별 토큰 개수의 분포 확인\n",
    "total_token_count = []\n",
    "for i in X:\n",
    "    total_token_count.append(len(i))\n",
    "\n",
    "plt.hist( total_token_count, bins='auto' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:20.890975Z",
     "start_time": "2021-05-11T15:52:20.817173Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.boxplot(total_token_count)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:23.242996Z",
     "start_time": "2021-05-11T15:52:23.237012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529, 506, 477, 321, 136, 105, 105, 102, 101, 100]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_token_count.sort(reverse=True)\n",
    "total_token_count[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 문장의 최대 토큰개수인 [955, 944, 842, 529, 228]개는 극단치라고 볼 수 있음\n",
    "- 토큰의 수가 [955, 944, 842, 529, 228]인 데이터 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:25.400250Z",
     "start_time": "2021-05-11T15:52:25.390278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len(tokens)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>21.856879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.303906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>529.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       len(tokens)\n",
       "count  4332.000000\n",
       "mean     21.856879\n",
       "std      19.303906\n",
       "min       3.000000\n",
       "25%      13.000000\n",
       "50%      17.000000\n",
       "75%      26.000000\n",
       "max     529.000000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 개수의 분포 확인\n",
    "pd.DataFrame(total_token_count, columns=['len(tokens)']).describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:27.559783Z",
     "start_time": "2021-05-11T15:52:27.553777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 개수의 95% 확인\n",
    "max_len = int(pd.Series(total_token_count).quantile(0.95) )\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **패딩(padding)** 시 맞춰주는 배열의 길이는 토큰의 최대길이 **163**로 맞춰준다.\n",
    "- **임베딩(embedding)** 시 배열을 압축할 때 사용하는 길이는 전체 문장의 95%가 포함되어 있는 토큰의 수인 **70**를 사용하여 압축한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 패딩(Padding)\n",
    "- **딥러닝 모델**에 **입력**을 하려면 **학습 데이터의 길이가 동일**해야함. 패딩은 길이를 맞춰주는 작업\n",
    "- 원하는 길이보다 짧은 부분은 숫자 0을 넣어 채우고,긴 데이터는 잘라서 같은 길이로 맞춘다.\n",
    "- 한 문장의 토큰 개수 합이 가장 많은 것으로 채우는 것이 좋다.\n",
    "- 따라서, 서로 다른 길이의 데이터를 **163**로 맞춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:36:36.084298Z",
     "start_time": "2021-05-11T15:36:36.058367Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  682,  877,   59],\n",
       "       [   0,    0,    0, ..., 1661,  107,   59],\n",
       "       [   0,    0,    0, ..., 1518, 2362, 4249],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  752,   30,  580],\n",
       "       [   0,    0,    0, ...,  829, 3269,   83],\n",
       "       [   0,    0,    0, ...,    8,  197,  506]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x = pad_sequences(X, max_len)\n",
    "padded_x # 배열의 길이가 맞춰짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 임베딩(Embedding)\n",
    "- **주어진 배열을 정해진 길이로 압축**\n",
    "- 텍스트를 원-핫 인코딩 시 벡터의 길이가 너무 길어지는 문제 해결\n",
    "\n",
    "\n",
    "- **Embedding**( 입력, 출력, 단어수 )\n",
    "    - **입력**: 총 몇 개의 단어 집합\n",
    "    - **출력**: 몇 개의 임베딩 결과 사용할 것인지(임의) -> 토큰 배열의 95%를 차지하는 70으로 설정\n",
    "    - **단어수**: 매번 입력될 단어 수는 몇 개로 할 것인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:36:38.881485Z",
     "start_time": "2021-05-11T15:36:38.876512Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30504"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 몇 개의 '인덱스'가 '입력' 되어야 하는지 정하기\n",
    "word_size = len(token.word_index) + 1 # 전체 단어 맨 앞에 0이 먼저 나와야 함.\n",
    "word_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 구조 생성/학습 및 저장\n",
    "- 1. 기본 딥러닝\n",
    "- 2. RNN(Recurrent Neural network, RNN)\n",
    "    - **여러개의 데이터가 순서대로 입력**되었을 때 앞서 입력받은 데이터를 **잠시 기억**해놓는 방법. 그리고 기억된 데이터가 얼마나 **중요한지를 판단**하여 **별도의 가중치**를 줘서 다음 데이터로 넘기게 됨\n",
    "    - **LSTM(Long Short Term Memory)**\n",
    "        - RNN의 기울기 소실 문제라는 단점을 보완한 방법\n",
    "        - 즉, 반복되기 직전에 다음 층으로 기억된 값을 넘길지 안 넘길지를 관리하는 단계 추가\n",
    "- 3. CNN과 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:36:41.175809Z",
     "start_time": "2021-05-11T15:36:41.172325Z"
    }
   },
   "outputs": [],
   "source": [
    "word_size = num_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:20:00.863309Z",
     "start_time": "2021-05-11T15:18:25.950588Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "87/87 [==============================] - 2s 7ms/step - loss: 1.5023 - accuracy: 0.3195 - val_loss: 1.2728 - val_accuracy: 0.4704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.27280, saving model to model/DNN_0511_2218\\1-1.2727967500686646.h5\n",
      "Epoch 2/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.2018 - accuracy: 0.5199 - val_loss: 1.1451 - val_accuracy: 0.5411\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.27280 to 1.14506, saving model to model/DNN_0511_2218\\2-1.1450629234313965.h5\n",
      "Epoch 3/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 1.0558 - accuracy: 0.6036 - val_loss: 0.9976 - val_accuracy: 0.6277\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.14506 to 0.99756, saving model to model/DNN_0511_2218\\3-0.997559666633606.h5\n",
      "Epoch 4/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.8602 - accuracy: 0.7178 - val_loss: 0.8761 - val_accuracy: 0.7215\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.99756 to 0.87610, saving model to model/DNN_0511_2218\\4-0.8760988116264343.h5\n",
      "Epoch 5/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.7202 - accuracy: 0.7994 - val_loss: 0.7783 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.87610 to 0.77825, saving model to model/DNN_0511_2218\\5-0.7782520651817322.h5\n",
      "Epoch 6/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.5786 - accuracy: 0.8533 - val_loss: 0.7120 - val_accuracy: 0.7807\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.77825 to 0.71196, saving model to model/DNN_0511_2218\\6-0.711961030960083.h5\n",
      "Epoch 7/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.4893 - accuracy: 0.8732 - val_loss: 0.6630 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.71196 to 0.66295, saving model to model/DNN_0511_2218\\7-0.6629502773284912.h5\n",
      "Epoch 8/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8789 - val_loss: 0.6255 - val_accuracy: 0.8153\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.66295 to 0.62554, saving model to model/DNN_0511_2218\\8-0.6255401372909546.h5\n",
      "Epoch 9/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.3536 - accuracy: 0.9104 - val_loss: 0.6022 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.62554 to 0.60225, saving model to model/DNN_0511_2218\\9-0.602249801158905.h5\n",
      "Epoch 10/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.9171 - val_loss: 0.5883 - val_accuracy: 0.8240\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.60225 to 0.58834, saving model to model/DNN_0511_2218\\10-0.5883377194404602.h5\n",
      "Epoch 11/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.2794 - accuracy: 0.9215 - val_loss: 0.5762 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.58834 to 0.57619, saving model to model/DNN_0511_2218\\11-0.5761889815330505.h5\n",
      "Epoch 12/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.2283 - accuracy: 0.9400 - val_loss: 0.5723 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.57619 to 0.57228, saving model to model/DNN_0511_2218\\12-0.5722842812538147.h5\n",
      "Epoch 13/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.2005 - accuracy: 0.9526 - val_loss: 0.5694 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.57228 to 0.56944, saving model to model/DNN_0511_2218\\13-0.5694381594657898.h5\n",
      "Epoch 14/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1636 - accuracy: 0.9633 - val_loss: 0.5697 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.56944\n",
      "Epoch 15/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1557 - accuracy: 0.9636 - val_loss: 0.5715 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.56944\n",
      "Epoch 16/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9627 - val_loss: 0.5762 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.56944\n",
      "Epoch 17/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 0.9734 - val_loss: 0.5824 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.56944\n",
      "Epoch 18/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.9704 - val_loss: 0.5922 - val_accuracy: 0.8196\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.56944\n",
      "Epoch 19/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9789 - val_loss: 0.5943 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.56944\n",
      "Epoch 20/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9801 - val_loss: 0.6003 - val_accuracy: 0.8167\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.56944\n",
      "Epoch 21/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9836 - val_loss: 0.6116 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.56944\n",
      "Epoch 22/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9825 - val_loss: 0.6159 - val_accuracy: 0.8110\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.56944\n",
      "Epoch 23/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9798 - val_loss: 0.6269 - val_accuracy: 0.8066\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.56944\n",
      "Epoch 24/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9841 - val_loss: 0.6340 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.56944\n",
      "Epoch 25/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.9850 - val_loss: 0.6402 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.56944\n",
      "Epoch 26/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9831 - val_loss: 0.6501 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.56944\n",
      "Epoch 27/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9869 - val_loss: 0.6562 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.56944\n",
      "Epoch 28/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9867 - val_loss: 0.6611 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.56944\n",
      "Epoch 29/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9867 - val_loss: 0.6752 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.56944\n",
      "Epoch 30/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9862 - val_loss: 0.6798 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.56944\n",
      "Epoch 31/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9899 - val_loss: 0.6852 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.56944\n",
      "Epoch 32/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9898 - val_loss: 0.6949 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.56944\n",
      "Epoch 33/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9923 - val_loss: 0.7041 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.56944\n",
      "Epoch 34/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0352 - accuracy: 0.9862 - val_loss: 0.7109 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.56944\n",
      "Epoch 35/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0401 - accuracy: 0.9842 - val_loss: 0.7176 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.56944\n",
      "Epoch 36/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.9910 - val_loss: 0.7258 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.56944\n",
      "Epoch 37/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.9876 - val_loss: 0.7385 - val_accuracy: 0.8038\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.56944\n",
      "Epoch 38/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.9897 - val_loss: 0.7381 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.56944\n",
      "Epoch 39/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9877 - val_loss: 0.7462 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.56944\n",
      "Epoch 40/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.9885 - val_loss: 0.7552 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.56944\n",
      "Epoch 41/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9890 - val_loss: 0.7585 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.56944\n",
      "Epoch 42/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.9892 - val_loss: 0.7726 - val_accuracy: 0.8023\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.56944\n",
      "Epoch 43/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.9875 - val_loss: 0.7747 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.56944\n",
      "Epoch 44/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.7844 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.56944\n",
      "Epoch 45/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9921 - val_loss: 0.7874 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.56944\n",
      "Epoch 46/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.9915 - val_loss: 0.7971 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.56944\n",
      "Epoch 47/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.9927 - val_loss: 0.8010 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.56944\n",
      "Epoch 48/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9917 - val_loss: 0.8063 - val_accuracy: 0.7994\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.56944\n",
      "Epoch 49/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.9919 - val_loss: 0.8113 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.56944\n",
      "Epoch 50/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.9865 - val_loss: 0.8157 - val_accuracy: 0.7951\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.56944\n",
      "Epoch 51/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.9871 - val_loss: 0.8264 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.56944\n",
      "Epoch 52/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9876 - val_loss: 0.8322 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.56944\n",
      "Epoch 53/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.9893 - val_loss: 0.8404 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.56944\n",
      "Epoch 54/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9886 - val_loss: 0.8514 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.56944\n",
      "Epoch 55/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.9900 - val_loss: 0.8519 - val_accuracy: 0.7951\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.56944\n",
      "Epoch 56/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.9883 - val_loss: 0.8484 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.56944\n",
      "Epoch 57/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.9892 - val_loss: 0.8670 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.56944\n",
      "Epoch 58/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9911 - val_loss: 0.8743 - val_accuracy: 0.7951\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.56944\n",
      "Epoch 59/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.9909 - val_loss: 0.8691 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.56944\n",
      "Epoch 60/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 0.9921 - val_loss: 0.8772 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.56944\n",
      "Epoch 61/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9905 - val_loss: 0.8867 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.56944\n",
      "Epoch 62/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.9895 - val_loss: 0.8980 - val_accuracy: 0.7937\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.56944\n",
      "Epoch 63/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9895 - val_loss: 0.8965 - val_accuracy: 0.7864\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.56944\n",
      "Epoch 64/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9880 - val_loss: 0.9016 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.56944\n",
      "Epoch 65/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.9897 - val_loss: 0.9095 - val_accuracy: 0.7937\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.56944\n",
      "Epoch 66/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.9893 - val_loss: 0.9096 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.56944\n",
      "Epoch 67/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.9877 - val_loss: 0.9172 - val_accuracy: 0.7821\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.56944\n",
      "Epoch 68/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 0.9924 - val_loss: 0.9239 - val_accuracy: 0.7835\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.56944\n",
      "Epoch 69/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.9901 - val_loss: 0.9232 - val_accuracy: 0.7835\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.56944\n",
      "Epoch 70/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9906 - val_loss: 0.9305 - val_accuracy: 0.7821\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.56944\n",
      "Epoch 71/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 0.9917 - val_loss: 0.9365 - val_accuracy: 0.7864\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.56944\n",
      "Epoch 72/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9867 - val_loss: 0.9444 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.56944\n",
      "Epoch 73/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9941 - val_loss: 0.9473 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.56944\n",
      "Epoch 74/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.9892 - val_loss: 0.9593 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.56944\n",
      "Epoch 75/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.9914 - val_loss: 0.9576 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.56944\n",
      "Epoch 76/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.9902 - val_loss: 0.9624 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.56944\n",
      "Epoch 77/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.9892 - val_loss: 0.9674 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.56944\n",
      "Epoch 78/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.9907 - val_loss: 0.9771 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.56944\n",
      "Epoch 79/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.9890 - val_loss: 0.9800 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.56944\n",
      "Epoch 80/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9940 - val_loss: 0.9827 - val_accuracy: 0.7821\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.56944\n",
      "Epoch 81/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.9865 - val_loss: 0.9845 - val_accuracy: 0.7778\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.56944\n",
      "Epoch 82/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.9861 - val_loss: 0.9899 - val_accuracy: 0.7763\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.56944\n",
      "Epoch 83/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.9885 - val_loss: 0.9981 - val_accuracy: 0.7792\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.56944\n",
      "Epoch 84/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9881 - val_loss: 1.0004 - val_accuracy: 0.7763\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.56944\n",
      "Epoch 85/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9935 - val_loss: 1.0006 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.56944\n",
      "Epoch 86/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 0.9901 - val_loss: 1.0084 - val_accuracy: 0.7763\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.56944\n",
      "Epoch 87/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 0.9884 - val_loss: 1.0190 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.56944\n",
      "Epoch 88/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.9902 - val_loss: 1.0231 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.56944\n",
      "Epoch 89/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.9899 - val_loss: 1.0296 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.56944\n",
      "Epoch 90/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9907 - val_loss: 1.0308 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.56944\n",
      "Epoch 91/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0157 - accuracy: 0.9896 - val_loss: 1.0345 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.56944\n",
      "Epoch 92/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 0.9915 - val_loss: 1.0356 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.56944\n",
      "Epoch 93/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9880 - val_loss: 1.0415 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.56944\n",
      "Epoch 94/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9933 - val_loss: 1.0540 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.56944\n",
      "Epoch 95/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 0.9905 - val_loss: 1.0670 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.56944\n",
      "Epoch 96/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0157 - accuracy: 0.9913 - val_loss: 1.0499 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.56944\n",
      "Epoch 97/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9889 - val_loss: 1.0637 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.56944\n",
      "Epoch 98/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.9889 - val_loss: 1.0669 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.56944\n",
      "Epoch 99/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 0.9883 - val_loss: 1.0771 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.56944\n",
      "Epoch 100/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9916 - val_loss: 1.0772 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.56944\n",
      "Epoch 101/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0154 - accuracy: 0.9895 - val_loss: 1.0786 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.56944\n",
      "Epoch 102/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.9832 - val_loss: 1.0813 - val_accuracy: 0.7691\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.56944\n",
      "Epoch 103/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9906 - val_loss: 1.0938 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.56944\n",
      "Epoch 104/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9915 - val_loss: 1.0962 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.56944\n",
      "Epoch 105/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 0.9924 - val_loss: 1.1017 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.56944\n",
      "Epoch 106/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.9869 - val_loss: 1.1078 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.56944\n",
      "Epoch 107/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9941 - val_loss: 1.1113 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.56944\n",
      "Epoch 108/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9919 - val_loss: 1.1170 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.56944\n",
      "Epoch 109/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.9910 - val_loss: 1.1167 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.56944\n",
      "Epoch 110/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 0.9898 - val_loss: 1.1291 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.56944\n",
      "Epoch 111/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9906 - val_loss: 1.1299 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.56944\n",
      "Epoch 112/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.9884 - val_loss: 1.1343 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.56944\n",
      "Epoch 113/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0146 - accuracy: 0.9893 - val_loss: 1.1376 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.56944\n",
      "Epoch 114/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 0.9900 - val_loss: 1.1462 - val_accuracy: 0.7734\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.56944\n",
      "Epoch 115/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9854 - val_loss: 1.1475 - val_accuracy: 0.7691\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.56944\n",
      "Epoch 116/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.9863 - val_loss: 1.1569 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.56944\n",
      "Epoch 117/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9926 - val_loss: 1.1643 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.56944\n",
      "Epoch 118/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9882 - val_loss: 1.1631 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.56944\n",
      "Epoch 119/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.9914 - val_loss: 1.1782 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.56944\n",
      "Epoch 120/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9910 - val_loss: 1.1724 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.56944\n",
      "Epoch 121/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9928 - val_loss: 1.1810 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.56944\n",
      "Epoch 122/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.9907 - val_loss: 1.1848 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.56944\n",
      "Epoch 123/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 0.9908 - val_loss: 1.1864 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.56944\n",
      "Epoch 124/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.9849 - val_loss: 1.1877 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.56944\n",
      "Epoch 125/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.9862 - val_loss: 1.1998 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.56944\n",
      "Epoch 126/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9849 - val_loss: 1.2051 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.56944\n",
      "Epoch 127/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9868 - val_loss: 1.2082 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.56944\n",
      "Epoch 128/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0160 - accuracy: 0.9888 - val_loss: 1.2176 - val_accuracy: 0.7677\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.56944\n",
      "Epoch 129/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9933 - val_loss: 1.2227 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.56944\n",
      "Epoch 130/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0157 - accuracy: 0.9907 - val_loss: 1.2283 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.56944\n",
      "Epoch 131/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.9889 - val_loss: 1.2329 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.56944\n",
      "Epoch 132/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0127 - accuracy: 0.9913 - val_loss: 1.2359 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.56944\n",
      "Epoch 133/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.9893 - val_loss: 1.2439 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.56944\n",
      "Epoch 134/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.9886 - val_loss: 1.2456 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.56944\n",
      "Epoch 135/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.9879 - val_loss: 1.2533 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.56944\n",
      "Epoch 136/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9877 - val_loss: 1.2547 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.56944\n",
      "Epoch 137/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 0.9883 - val_loss: 1.2607 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.56944\n",
      "Epoch 138/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.9839 - val_loss: 1.2615 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.56944\n",
      "Epoch 139/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.9862 - val_loss: 1.2713 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.56944\n",
      "Epoch 140/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9880 - val_loss: 1.2797 - val_accuracy: 0.7590\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.56944\n",
      "Epoch 141/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.9854 - val_loss: 1.2852 - val_accuracy: 0.7605\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.56944\n",
      "Epoch 142/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9877 - val_loss: 1.2911 - val_accuracy: 0.7590\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.56944\n",
      "Epoch 143/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0154 - accuracy: 0.9876 - val_loss: 1.2997 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.56944\n",
      "Epoch 144/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 0.9873 - val_loss: 1.3015 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.56944\n",
      "Epoch 145/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9896 - val_loss: 1.3039 - val_accuracy: 0.7590\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.56944\n",
      "Epoch 146/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9911 - val_loss: 1.3056 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.56944\n",
      "Epoch 147/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.9913 - val_loss: 1.3113 - val_accuracy: 0.7590\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.56944\n",
      "Epoch 148/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9932 - val_loss: 1.3149 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.56944\n",
      "Epoch 149/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.9872 - val_loss: 1.3225 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.56944\n",
      "Epoch 150/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 0.9884 - val_loss: 1.3311 - val_accuracy: 0.7590\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.56944\n",
      "Epoch 151/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9901 - val_loss: 1.3382 - val_accuracy: 0.7590\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.56944\n",
      "Epoch 152/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9899 - val_loss: 1.3394 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.56944\n",
      "Epoch 153/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9894 - val_loss: 1.3451 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.56944\n",
      "Epoch 154/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9917 - val_loss: 1.3510 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.56944\n",
      "Epoch 155/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9885 - val_loss: 1.3541 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.56944\n",
      "Epoch 156/1500\n",
      "87/87 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.9938 - val_loss: 1.3584 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.56944\n",
      "Epoch 157/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9912 - val_loss: 1.3635 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.56944\n",
      "Epoch 158/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9926 - val_loss: 1.3673 - val_accuracy: 0.7561\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.56944\n",
      "Epoch 159/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9898 - val_loss: 1.3824 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.56944\n",
      "Epoch 160/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9910 - val_loss: 1.3875 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.56944\n",
      "Epoch 161/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9913 - val_loss: 1.3907 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.56944\n",
      "Epoch 162/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9880 - val_loss: 1.3971 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.56944\n",
      "Epoch 163/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9894 - val_loss: 1.4092 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.56944\n",
      "Epoch 164/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 0.9945 - val_loss: 1.4058 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.56944\n",
      "Epoch 165/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.9865 - val_loss: 1.4150 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.56944\n",
      "Epoch 166/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9906 - val_loss: 1.4203 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.56944\n",
      "Epoch 167/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9881 - val_loss: 1.4308 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.56944\n",
      "Epoch 168/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9936 - val_loss: 1.4317 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.56944\n",
      "Epoch 169/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9876 - val_loss: 1.4316 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.56944\n",
      "Epoch 170/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9917 - val_loss: 1.4412 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.56944\n",
      "Epoch 171/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9892 - val_loss: 1.4471 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.56944\n",
      "Epoch 172/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.9856 - val_loss: 1.4521 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.56944\n",
      "Epoch 173/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9866 - val_loss: 1.4571 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.56944\n",
      "Epoch 174/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9944 - val_loss: 1.4630 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.56944\n",
      "Epoch 175/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.9848 - val_loss: 1.4669 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.56944\n",
      "Epoch 176/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9913 - val_loss: 1.4730 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.56944\n",
      "Epoch 177/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9931 - val_loss: 1.4787 - val_accuracy: 0.7547\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.56944\n",
      "Epoch 178/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9887 - val_loss: 1.4867 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.56944\n",
      "Epoch 179/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9871 - val_loss: 1.4931 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.56944\n",
      "Epoch 180/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9888 - val_loss: 1.4988 - val_accuracy: 0.7532\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.56944\n",
      "Epoch 181/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9890 - val_loss: 1.5047 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.56944\n",
      "Epoch 182/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.9846 - val_loss: 1.5122 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.56944\n",
      "Epoch 183/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.9853 - val_loss: 1.5161 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.56944\n",
      "Epoch 184/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9914 - val_loss: 1.5180 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.56944\n",
      "Epoch 185/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0129 - accuracy: 0.9907 - val_loss: 1.5275 - val_accuracy: 0.7518\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.56944\n",
      "Epoch 186/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9884 - val_loss: 1.5271 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.56944\n",
      "Epoch 187/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9889 - val_loss: 1.5335 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.56944\n",
      "Epoch 188/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9906 - val_loss: 1.5379 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.56944\n",
      "Epoch 189/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.9861 - val_loss: 1.5448 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.56944\n",
      "Epoch 190/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9894 - val_loss: 1.5490 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.56944\n",
      "Epoch 191/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9866 - val_loss: 1.5614 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.56944\n",
      "Epoch 192/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9910 - val_loss: 1.5650 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.56944\n",
      "Epoch 193/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9901 - val_loss: 1.5752 - val_accuracy: 0.7460\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.56944\n",
      "Epoch 194/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9864 - val_loss: 1.5792 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.56944\n",
      "Epoch 195/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.9820 - val_loss: 1.5845 - val_accuracy: 0.7460\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.56944\n",
      "Epoch 196/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9914 - val_loss: 1.5932 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.56944\n",
      "Epoch 197/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9889 - val_loss: 1.5912 - val_accuracy: 0.7446\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.56944\n",
      "Epoch 198/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 0.9900 - val_loss: 1.6023 - val_accuracy: 0.7460\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.56944\n",
      "Epoch 199/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0134 - accuracy: 0.9905 - val_loss: 1.6062 - val_accuracy: 0.7460\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.56944\n",
      "Epoch 200/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.9873 - val_loss: 1.6176 - val_accuracy: 0.7460\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.56944\n",
      "Epoch 201/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.9925 - val_loss: 1.6148 - val_accuracy: 0.7431\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.56944\n",
      "Epoch 202/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.9821 - val_loss: 1.6234 - val_accuracy: 0.7417\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.56944\n",
      "Epoch 203/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0144 - accuracy: 0.9920 - val_loss: 1.6326 - val_accuracy: 0.7475\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.56944\n",
      "Epoch 204/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 0.9874 - val_loss: 1.6346 - val_accuracy: 0.7431\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.56944\n",
      "Epoch 205/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.9921 - val_loss: 1.6358 - val_accuracy: 0.7417\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.56944\n",
      "Epoch 206/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.9856 - val_loss: 1.6434 - val_accuracy: 0.7417\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.56944\n",
      "Epoch 207/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 0.9909 - val_loss: 1.6524 - val_accuracy: 0.7446\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.56944\n",
      "Epoch 208/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.9913 - val_loss: 1.6545 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.56944\n",
      "Epoch 209/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 0.9902 - val_loss: 1.6597 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.56944\n",
      "Epoch 210/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9906 - val_loss: 1.6686 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.56944\n",
      "Epoch 211/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0154 - accuracy: 0.9919 - val_loss: 1.6739 - val_accuracy: 0.7431\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.56944\n",
      "Epoch 212/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9884 - val_loss: 1.6752 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.56944\n",
      "Epoch 213/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.9864 - val_loss: 1.6803 - val_accuracy: 0.7417\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.56944\n",
      "Epoch 214/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9885 - val_loss: 1.6907 - val_accuracy: 0.7431\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.56944\n",
      "Epoch 215/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9902 - val_loss: 1.6892 - val_accuracy: 0.7431\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.56944\n",
      "Epoch 216/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9909 - val_loss: 1.7028 - val_accuracy: 0.7460\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.56944\n",
      "Epoch 217/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9922 - val_loss: 1.7044 - val_accuracy: 0.7431\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.56944\n",
      "Epoch 218/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 0.9924 - val_loss: 1.7118 - val_accuracy: 0.7417\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.56944\n",
      "Epoch 219/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9867 - val_loss: 1.7150 - val_accuracy: 0.7417\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.56944\n",
      "Epoch 220/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9919 - val_loss: 1.7247 - val_accuracy: 0.7417\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.56944\n",
      "Epoch 221/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9850 - val_loss: 1.7299 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.56944\n",
      "Epoch 222/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9927 - val_loss: 1.7334 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.56944\n",
      "Epoch 223/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9865 - val_loss: 1.7354 - val_accuracy: 0.7388\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.56944\n",
      "Epoch 224/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.9862 - val_loss: 1.7452 - val_accuracy: 0.7388\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.56944\n",
      "Epoch 225/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0150 - accuracy: 0.9889 - val_loss: 1.7471 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.56944\n",
      "Epoch 226/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.9866 - val_loss: 1.7547 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.56944\n",
      "Epoch 227/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9928 - val_loss: 1.7603 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.56944\n",
      "Epoch 228/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9904 - val_loss: 1.7634 - val_accuracy: 0.7388\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.56944\n",
      "Epoch 229/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9887 - val_loss: 1.7713 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.56944\n",
      "Epoch 230/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0166 - accuracy: 0.9897 - val_loss: 1.7782 - val_accuracy: 0.7403\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.56944\n",
      "Epoch 231/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.9846 - val_loss: 1.7842 - val_accuracy: 0.7359\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.56944\n",
      "Epoch 232/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9867 - val_loss: 1.7918 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.56944\n",
      "Epoch 233/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9899 - val_loss: 1.7949 - val_accuracy: 0.7345\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.56944\n",
      "Epoch 234/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0154 - accuracy: 0.9866 - val_loss: 1.7947 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.56944\n",
      "Epoch 235/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.9917 - val_loss: 1.8052 - val_accuracy: 0.7359\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.56944\n",
      "Epoch 236/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9894 - val_loss: 1.8101 - val_accuracy: 0.7345\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.56944\n",
      "Epoch 237/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9893 - val_loss: 1.8175 - val_accuracy: 0.7345\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.56944\n",
      "Epoch 238/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.9867 - val_loss: 1.8234 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.56944\n",
      "Epoch 239/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 0.9881 - val_loss: 1.8251 - val_accuracy: 0.7359\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.56944\n",
      "Epoch 240/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0159 - accuracy: 0.9867 - val_loss: 1.8324 - val_accuracy: 0.7359\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.56944\n",
      "Epoch 241/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 0.9883 - val_loss: 1.8327 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.56944\n",
      "Epoch 242/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.9888 - val_loss: 1.8446 - val_accuracy: 0.7316\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.56944\n",
      "Epoch 243/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0157 - accuracy: 0.9898 - val_loss: 1.8484 - val_accuracy: 0.7316\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.56944\n",
      "Epoch 244/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.9865 - val_loss: 1.8566 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.56944\n",
      "Epoch 245/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 0.9926 - val_loss: 1.8535 - val_accuracy: 0.7302\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.56944\n",
      "Epoch 246/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0118 - accuracy: 0.9913 - val_loss: 1.8595 - val_accuracy: 0.7316\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.56944\n",
      "Epoch 247/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9899 - val_loss: 1.8693 - val_accuracy: 0.7345\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.56944\n",
      "Epoch 248/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.9898 - val_loss: 1.8686 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.56944\n",
      "Epoch 249/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 0.9891 - val_loss: 1.8771 - val_accuracy: 0.7302\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.56944\n",
      "Epoch 250/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 0.9900 - val_loss: 1.8830 - val_accuracy: 0.7302\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.56944\n",
      "Epoch 251/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.9889 - val_loss: 1.8881 - val_accuracy: 0.7302\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.56944\n",
      "Epoch 252/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.9893 - val_loss: 1.8910 - val_accuracy: 0.7316\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.56944\n",
      "Epoch 253/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9874 - val_loss: 1.8955 - val_accuracy: 0.7273\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.56944\n",
      "Epoch 254/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 0.9891 - val_loss: 1.9034 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.56944\n",
      "Epoch 255/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 0.9873 - val_loss: 1.9118 - val_accuracy: 0.7287\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.56944\n",
      "Epoch 256/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0146 - accuracy: 0.9895 - val_loss: 1.9122 - val_accuracy: 0.7273\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.56944\n",
      "Epoch 257/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0143 - accuracy: 0.9908 - val_loss: 1.9204 - val_accuracy: 0.7287\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.56944\n",
      "Epoch 258/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 0.9906 - val_loss: 1.9201 - val_accuracy: 0.7273\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.56944\n",
      "Epoch 259/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9894 - val_loss: 1.9252 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.56944\n",
      "Epoch 260/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.9931 - val_loss: 1.9342 - val_accuracy: 0.7273\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.56944\n",
      "Epoch 261/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 0.9880 - val_loss: 1.9374 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.56944\n",
      "Epoch 262/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9895 - val_loss: 1.9430 - val_accuracy: 0.7244\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.56944\n",
      "Epoch 263/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0150 - accuracy: 0.9881 - val_loss: 1.9471 - val_accuracy: 0.7229\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.56944\n",
      "Epoch 264/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.9870 - val_loss: 1.9494 - val_accuracy: 0.7229\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.56944\n",
      "Epoch 265/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9917 - val_loss: 1.9546 - val_accuracy: 0.7215\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.56944\n",
      "Epoch 266/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9893 - val_loss: 1.9613 - val_accuracy: 0.7229\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.56944\n",
      "Epoch 267/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 0.9907 - val_loss: 1.9651 - val_accuracy: 0.7215\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.56944\n",
      "Epoch 268/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9909 - val_loss: 1.9686 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.56944\n",
      "Epoch 269/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9909 - val_loss: 1.9712 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.56944\n",
      "Epoch 270/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9884 - val_loss: 1.9784 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.56944\n",
      "Epoch 271/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.9843 - val_loss: 1.9809 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.56944\n",
      "Epoch 272/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 0.9922 - val_loss: 1.9852 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.56944\n",
      "Epoch 273/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 0.9906 - val_loss: 1.9909 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.56944\n",
      "Epoch 274/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9929 - val_loss: 1.9983 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.56944\n",
      "Epoch 275/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0146 - accuracy: 0.9928 - val_loss: 1.9978 - val_accuracy: 0.7157\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.56944\n",
      "Epoch 276/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0136 - accuracy: 0.9905 - val_loss: 2.0041 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.56944\n",
      "Epoch 277/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.9869 - val_loss: 2.0103 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.56944\n",
      "Epoch 278/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 0.9909 - val_loss: 2.0130 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.56944\n",
      "Epoch 279/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0125 - accuracy: 0.9883 - val_loss: 2.0189 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.56944\n",
      "Epoch 280/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0080 - accuracy: 0.9952 - val_loss: 2.0223 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.56944\n",
      "Epoch 281/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9836 - val_loss: 2.0234 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.56944\n",
      "Epoch 282/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 0.9912 - val_loss: 2.0274 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.56944\n",
      "Epoch 283/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.9863 - val_loss: 2.0317 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.56944\n",
      "Epoch 284/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9912 - val_loss: 2.0368 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.56944\n",
      "Epoch 285/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.9890 - val_loss: 2.0416 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.56944\n",
      "Epoch 286/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 0.9895 - val_loss: 2.0497 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.56944\n",
      "Epoch 287/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 0.9910 - val_loss: 2.0486 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.56944\n",
      "Epoch 288/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 0.9919 - val_loss: 2.0537 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.56944\n",
      "Epoch 289/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 0.9917 - val_loss: 2.0547 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.56944\n",
      "Epoch 290/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9832 - val_loss: 2.0600 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.56944\n",
      "Epoch 291/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9880 - val_loss: 2.0626 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.56944\n",
      "Epoch 292/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9926 - val_loss: 2.0676 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.56944\n",
      "Epoch 293/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9872 - val_loss: 2.0697 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.56944\n",
      "Epoch 294/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 0.9917 - val_loss: 2.0739 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.56944\n",
      "Epoch 295/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.9877 - val_loss: 2.0802 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.56944\n",
      "Epoch 296/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9893 - val_loss: 2.0835 - val_accuracy: 0.7201\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.56944\n",
      "Epoch 297/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.9870 - val_loss: 2.0868 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.56944\n",
      "Epoch 298/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9880 - val_loss: 2.0940 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.56944\n",
      "Epoch 299/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9915 - val_loss: 2.0961 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.56944\n",
      "Epoch 300/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0120 - accuracy: 0.9919 - val_loss: 2.1003 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.56944\n",
      "Epoch 301/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0152 - accuracy: 0.9902 - val_loss: 2.1021 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.56944\n",
      "Epoch 302/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9908 - val_loss: 2.1062 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.56944\n",
      "Epoch 303/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9892 - val_loss: 2.1123 - val_accuracy: 0.7172\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.56944\n",
      "Epoch 304/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9898 - val_loss: 2.1180 - val_accuracy: 0.7114\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.56944\n",
      "Epoch 305/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9914 - val_loss: 2.1213 - val_accuracy: 0.7157\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.56944\n",
      "Epoch 306/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9889 - val_loss: 2.1213 - val_accuracy: 0.7157\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.56944\n",
      "Epoch 307/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9920 - val_loss: 2.1240 - val_accuracy: 0.7157\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.56944\n",
      "Epoch 308/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.9881 - val_loss: 2.1267 - val_accuracy: 0.7100\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.56944\n",
      "Epoch 309/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 0.9922 - val_loss: 2.1278 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.56944\n",
      "Epoch 310/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9875 - val_loss: 2.1349 - val_accuracy: 0.7157\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.56944\n",
      "Epoch 311/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.0152 - accuracy: 0.9910 - val_loss: 2.1389 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.56944\n",
      "Epoch 312/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9886 - val_loss: 2.1403 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.56944\n",
      "Epoch 313/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 0.9909 - val_loss: 2.1370 - val_accuracy: 0.7114\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.56944\n"
     ]
    }
   ],
   "source": [
    "# DNN 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding( num_word, 8 , input_length=max_len ) )\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='softmax')) # 다중분류\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile( loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "model_path = 'model/DNN_0511_2218/{epoch}-{val_loss}.h5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', \n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',patience=300)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_x, \n",
    "                                                     y_encoded, \n",
    "                                                     test_size=0.2, \n",
    "                                                     stratify=y_encoded)\n",
    "\n",
    "\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit( X_train, y_train, validation_split=0.2,\n",
    "                    epochs=1500,\n",
    "                    callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:20:00.869293Z",
     "start_time": "2021-05-11T15:20:00.865305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 43, 8)             24000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 344)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1725      \n",
      "=================================================================\n",
      "Total params: 25,725\n",
      "Trainable params: 25,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:20:01.017921Z",
     "start_time": "2021-05-11T15:20:00.871289Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트셋으로 실험 결과의 오차값을 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋으로 측정한 정확도의 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "# x값을 지정하고 그래프로 확인\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.title('[Figure] DNN 모델의 결과')\n",
    "plt.plot( x_len, y_vloss, 'o', c='red', markersize=3, label='TestSet Loss')\n",
    "plt.plot(x_len, y_acc, 'o', c='blue', markersize=3, label='TrainSet Acuraccy')\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:20:01.305643Z",
     "start_time": "2021-05-11T15:20:01.018894Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.9354\n",
      "0.9353535175323486\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 2.5898 - accuracy: 0.6863\n",
      "0.686274528503418\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 정확도\n",
    "print(model.evaluate(X_train,y_train)[1])\n",
    "\n",
    "# 테스트 데이터 정확도\n",
    "print(model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:20:01.309631Z",
     "start_time": "2021-05-11T15:20:01.306639Z"
    }
   },
   "outputs": [],
   "source": [
    "# 불러올 총 단어의 수\n",
    "word_size = num_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:20:01.339551Z",
     "start_time": "2021-05-11T15:20:01.310630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  682,  877,   59],\n",
       "       [   0,    0,    0, ..., 1661,  107,   59],\n",
       "       [   0,    0,    0, ...,  301, 1518, 2362],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  752,   30,  580],\n",
       "       [   0,    0,    0, ...,  632,  829,   83],\n",
       "       [   0,    0,    0, ...,    8,  197,  506]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x = pad_sequences(X, max_len)  # 토큰의 95%\n",
    "padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:23:00.454264Z",
     "start_time": "2021-05-11T15:20:01.340549Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "67/67 [==============================] - 3s 13ms/step - loss: 1.4456 - accuracy: 0.3087 - val_loss: 1.2280 - val_accuracy: 0.5703\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.22798, saving model to model/LSTM_0511_2218\\1-1.2279846668243408.h5\n",
      "Epoch 2/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 1.0601 - accuracy: 0.6233 - val_loss: 0.7734 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.22798 to 0.77337, saving model to model/LSTM_0511_2218\\2-0.7733738422393799.h5\n",
      "Epoch 3/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.5926 - accuracy: 0.8102 - val_loss: 0.6903 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.77337 to 0.69026, saving model to model/LSTM_0511_2218\\3-0.6902602910995483.h5\n",
      "Epoch 4/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.4337 - accuracy: 0.8589 - val_loss: 0.7209 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69026\n",
      "Epoch 5/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.3589 - accuracy: 0.8821 - val_loss: 0.7540 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69026\n",
      "Epoch 6/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.2613 - accuracy: 0.9253 - val_loss: 0.7479 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69026\n",
      "Epoch 7/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.2171 - accuracy: 0.9351 - val_loss: 0.8038 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69026\n",
      "Epoch 8/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.1543 - accuracy: 0.9550 - val_loss: 0.8472 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69026\n",
      "Epoch 9/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.1429 - accuracy: 0.9568 - val_loss: 0.9144 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69026\n",
      "Epoch 10/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.1146 - accuracy: 0.9707 - val_loss: 0.9701 - val_accuracy: 0.7505\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69026\n",
      "Epoch 11/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0977 - accuracy: 0.9754 - val_loss: 1.0006 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69026\n",
      "Epoch 12/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0999 - accuracy: 0.9729 - val_loss: 1.0131 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69026\n",
      "Epoch 13/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0797 - accuracy: 0.9764 - val_loss: 1.0390 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69026\n",
      "Epoch 14/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0639 - accuracy: 0.9863 - val_loss: 1.0894 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69026\n",
      "Epoch 15/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0512 - accuracy: 0.9857 - val_loss: 1.0845 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.69026\n",
      "Epoch 16/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0591 - accuracy: 0.9805 - val_loss: 1.2747 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.69026\n",
      "Epoch 17/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0532 - accuracy: 0.9835 - val_loss: 1.1489 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.69026\n",
      "Epoch 18/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 1.1558 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.69026\n",
      "Epoch 19/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0446 - accuracy: 0.9855 - val_loss: 1.1673 - val_accuracy: 0.7505\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.69026\n",
      "Epoch 20/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0446 - accuracy: 0.9823 - val_loss: 1.2266 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.69026\n",
      "Epoch 21/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0381 - accuracy: 0.9858 - val_loss: 1.2597 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.69026\n",
      "Epoch 22/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0337 - accuracy: 0.9890 - val_loss: 1.3110 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.69026\n",
      "Epoch 23/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0353 - accuracy: 0.9861 - val_loss: 1.2661 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.69026\n",
      "Epoch 24/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0303 - accuracy: 0.9882 - val_loss: 1.3217 - val_accuracy: 0.7495\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.69026\n",
      "Epoch 25/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0251 - accuracy: 0.9915 - val_loss: 1.3311 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.69026\n",
      "Epoch 26/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0259 - accuracy: 0.9886 - val_loss: 1.3721 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.69026\n",
      "Epoch 27/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0694 - accuracy: 0.9798 - val_loss: 1.1501 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.69026\n",
      "Epoch 28/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0491 - accuracy: 0.9823 - val_loss: 1.1648 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.69026\n",
      "Epoch 29/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0353 - accuracy: 0.9877 - val_loss: 1.2776 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.69026\n",
      "Epoch 30/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0218 - accuracy: 0.9945 - val_loss: 1.3315 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.69026\n",
      "Epoch 31/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0237 - accuracy: 0.9905 - val_loss: 1.3691 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.69026\n",
      "Epoch 32/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0193 - accuracy: 0.9910 - val_loss: 1.4147 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.69026\n",
      "Epoch 33/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0220 - accuracy: 0.9926 - val_loss: 1.3988 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.69026\n",
      "Epoch 34/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0234 - accuracy: 0.9920 - val_loss: 1.4269 - val_accuracy: 0.7495\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.69026\n",
      "Epoch 35/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0235 - accuracy: 0.9879 - val_loss: 1.4520 - val_accuracy: 0.7505\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.69026\n",
      "Epoch 36/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0204 - accuracy: 0.9905 - val_loss: 1.4899 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.69026\n",
      "Epoch 37/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0224 - accuracy: 0.9916 - val_loss: 1.5098 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.69026\n",
      "Epoch 38/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0198 - accuracy: 0.9894 - val_loss: 1.5389 - val_accuracy: 0.7154\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.69026\n",
      "Epoch 39/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0203 - accuracy: 0.9923 - val_loss: 1.5473 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.69026\n",
      "Epoch 40/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0180 - accuracy: 0.9909 - val_loss: 1.5568 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.69026\n",
      "Epoch 41/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0152 - accuracy: 0.9937 - val_loss: 1.5614 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.69026\n",
      "Epoch 42/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0185 - accuracy: 0.9904 - val_loss: 1.5684 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.69026\n",
      "Epoch 43/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0138 - accuracy: 0.9919 - val_loss: 1.6093 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.69026\n",
      "Epoch 44/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0210 - accuracy: 0.9911 - val_loss: 1.6052 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.69026\n",
      "Epoch 45/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0176 - accuracy: 0.9900 - val_loss: 1.6186 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.69026\n",
      "Epoch 46/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0174 - accuracy: 0.9919 - val_loss: 1.6342 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.69026\n",
      "Epoch 47/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0162 - accuracy: 0.9923 - val_loss: 1.6549 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.69026\n",
      "Epoch 48/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0130 - accuracy: 0.9953 - val_loss: 1.6548 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.69026\n",
      "Epoch 49/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0167 - accuracy: 0.9902 - val_loss: 1.6710 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.69026\n",
      "Epoch 50/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0145 - accuracy: 0.9898 - val_loss: 1.6874 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.69026\n",
      "Epoch 51/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0164 - accuracy: 0.9909 - val_loss: 1.6748 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.69026\n",
      "Epoch 52/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0159 - accuracy: 0.9927 - val_loss: 1.7125 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.69026\n",
      "Epoch 53/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0228 - accuracy: 0.9872 - val_loss: 1.7192 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.69026\n",
      "Epoch 54/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0127 - accuracy: 0.9935 - val_loss: 1.7234 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.69026\n",
      "Epoch 55/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0167 - accuracy: 0.9934 - val_loss: 1.7326 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.69026\n",
      "Epoch 56/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0140 - accuracy: 0.9943 - val_loss: 1.7340 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.69026\n",
      "Epoch 57/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0116 - accuracy: 0.9944 - val_loss: 1.7621 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.69026\n",
      "Epoch 58/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0147 - accuracy: 0.9912 - val_loss: 1.7655 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.69026\n",
      "Epoch 59/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0140 - accuracy: 0.9915 - val_loss: 1.7765 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.69026\n",
      "Epoch 60/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0169 - accuracy: 0.9930 - val_loss: 1.7508 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.69026\n",
      "Epoch 61/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0133 - accuracy: 0.9901 - val_loss: 1.7758 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.69026\n",
      "Epoch 62/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0155 - accuracy: 0.9886 - val_loss: 1.7812 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.69026\n",
      "Epoch 63/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0157 - accuracy: 0.9902 - val_loss: 1.8132 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.69026\n",
      "Epoch 64/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0099 - accuracy: 0.9952 - val_loss: 1.8240 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.69026\n",
      "Epoch 65/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0177 - accuracy: 0.9875 - val_loss: 1.8047 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.69026\n",
      "Epoch 66/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0133 - accuracy: 0.9944 - val_loss: 1.7983 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.69026\n",
      "Epoch 67/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0152 - accuracy: 0.9938 - val_loss: 2.0286 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.69026\n",
      "Epoch 68/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.1021 - accuracy: 0.9636 - val_loss: 1.4588 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.69026\n",
      "Epoch 69/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0419 - accuracy: 0.9825 - val_loss: 1.4249 - val_accuracy: 0.6846\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.69026\n",
      "Epoch 70/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0238 - accuracy: 0.9910 - val_loss: 1.4792 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.69026\n",
      "Epoch 71/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0273 - accuracy: 0.9875 - val_loss: 1.4600 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.69026\n",
      "Epoch 72/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0153 - accuracy: 0.9947 - val_loss: 1.5015 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.69026\n",
      "Epoch 73/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0181 - accuracy: 0.9877 - val_loss: 1.5281 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.69026\n",
      "Epoch 74/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0134 - accuracy: 0.9918 - val_loss: 1.5611 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.69026\n",
      "Epoch 75/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0149 - accuracy: 0.9889 - val_loss: 1.5948 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.69026\n",
      "Epoch 76/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0128 - accuracy: 0.9929 - val_loss: 1.6107 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.69026\n",
      "Epoch 77/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0106 - accuracy: 0.9950 - val_loss: 1.6391 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.69026\n",
      "Epoch 78/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0114 - accuracy: 0.9936 - val_loss: 1.6618 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.69026\n",
      "Epoch 79/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0145 - accuracy: 0.9903 - val_loss: 1.6634 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.69026\n",
      "Epoch 80/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0118 - accuracy: 0.9939 - val_loss: 1.7137 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.69026\n",
      "Epoch 81/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0123 - accuracy: 0.9911 - val_loss: 1.6945 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.69026\n",
      "Epoch 82/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0100 - accuracy: 0.9945 - val_loss: 1.7410 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.69026\n",
      "Epoch 83/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0112 - accuracy: 0.9934 - val_loss: 1.7163 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.69026\n",
      "Epoch 84/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0119 - accuracy: 0.9947 - val_loss: 1.7464 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.69026\n",
      "Epoch 85/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0164 - accuracy: 0.9882 - val_loss: 1.7563 - val_accuracy: 0.7319\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.69026\n",
      "Epoch 86/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0106 - accuracy: 0.9941 - val_loss: 1.7544 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.69026\n",
      "Epoch 87/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0127 - accuracy: 0.9924 - val_loss: 1.7883 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.69026\n",
      "Epoch 88/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0096 - accuracy: 0.9958 - val_loss: 1.7884 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.69026\n",
      "Epoch 89/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0122 - accuracy: 0.9908 - val_loss: 1.8220 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.69026\n",
      "Epoch 90/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0151 - accuracy: 0.9876 - val_loss: 1.7960 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.69026\n",
      "Epoch 91/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0122 - accuracy: 0.9906 - val_loss: 1.8137 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.69026\n",
      "Epoch 92/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0101 - accuracy: 0.9928 - val_loss: 1.8332 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.69026\n",
      "Epoch 93/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0151 - accuracy: 0.9864 - val_loss: 1.8365 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.69026\n",
      "Epoch 94/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0113 - accuracy: 0.9915 - val_loss: 1.8625 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.69026\n",
      "Epoch 95/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0109 - accuracy: 0.9910 - val_loss: 1.8633 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.69026\n",
      "Epoch 96/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0099 - accuracy: 0.9948 - val_loss: 1.8650 - val_accuracy: 0.7319\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.69026\n",
      "Epoch 97/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0106 - accuracy: 0.9924 - val_loss: 1.8715 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.69026\n",
      "Epoch 98/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0148 - accuracy: 0.9885 - val_loss: 1.8698 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.69026\n",
      "Epoch 99/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0109 - accuracy: 0.9937 - val_loss: 1.9001 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.69026\n",
      "Epoch 100/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0113 - accuracy: 0.9930 - val_loss: 1.8985 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.69026\n",
      "Epoch 101/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0106 - accuracy: 0.9939 - val_loss: 1.9138 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.69026\n",
      "Epoch 102/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0129 - accuracy: 0.9896 - val_loss: 1.9221 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.69026\n",
      "Epoch 103/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0114 - accuracy: 0.9949 - val_loss: 1.9114 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.69026\n",
      "Epoch 104/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0174 - accuracy: 0.9887 - val_loss: 1.9284 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.69026\n",
      "Epoch 105/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0158 - accuracy: 0.9919 - val_loss: 1.9562 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.69026\n",
      "Epoch 106/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0122 - accuracy: 0.9906 - val_loss: 1.9786 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.69026\n",
      "Epoch 107/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0099 - accuracy: 0.9928 - val_loss: 1.9607 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.69026\n",
      "Epoch 108/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0102 - accuracy: 0.9970 - val_loss: 1.9814 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.69026\n",
      "Epoch 109/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0112 - accuracy: 0.9931 - val_loss: 1.9652 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.69026\n",
      "Epoch 110/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0104 - accuracy: 0.9938 - val_loss: 1.9853 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.69026\n",
      "Epoch 111/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0081 - accuracy: 0.9943 - val_loss: 1.9982 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.69026\n",
      "Epoch 112/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0098 - accuracy: 0.9929 - val_loss: 2.0021 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.69026\n",
      "Epoch 113/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0128 - accuracy: 0.9913 - val_loss: 2.0171 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.69026\n",
      "Epoch 114/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0116 - accuracy: 0.9911 - val_loss: 2.0179 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.69026\n",
      "Epoch 115/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0128 - accuracy: 0.9913 - val_loss: 2.0174 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.69026\n",
      "Epoch 116/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0117 - accuracy: 0.9922 - val_loss: 2.0232 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.69026\n",
      "Epoch 117/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0111 - accuracy: 0.9911 - val_loss: 2.0175 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.69026\n",
      "Epoch 118/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0106 - accuracy: 0.9928 - val_loss: 2.0275 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.69026\n",
      "Epoch 119/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0128 - accuracy: 0.9915 - val_loss: 2.0255 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.69026\n",
      "Epoch 120/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0101 - accuracy: 0.9926 - val_loss: 2.0452 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.69026\n",
      "Epoch 121/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0113 - accuracy: 0.9937 - val_loss: 2.0507 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.69026\n",
      "Epoch 122/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0099 - accuracy: 0.9930 - val_loss: 2.0590 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.69026\n",
      "Epoch 123/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0117 - accuracy: 0.9918 - val_loss: 2.0509 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.69026\n",
      "Epoch 124/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0130 - accuracy: 0.9927 - val_loss: 2.0493 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.69026\n",
      "Epoch 125/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0088 - accuracy: 0.9931 - val_loss: 2.0622 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.69026\n",
      "Epoch 126/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0109 - accuracy: 0.9904 - val_loss: 2.0637 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.69026\n",
      "Epoch 127/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0118 - accuracy: 0.9909 - val_loss: 2.0617 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.69026\n",
      "Epoch 128/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0103 - accuracy: 0.9922 - val_loss: 2.0829 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.69026\n",
      "Epoch 129/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0111 - accuracy: 0.9924 - val_loss: 2.0885 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.69026\n",
      "Epoch 130/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0122 - accuracy: 0.9922 - val_loss: 2.0951 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.69026\n",
      "Epoch 131/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0089 - accuracy: 0.9932 - val_loss: 2.0917 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.69026\n",
      "Epoch 132/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0073 - accuracy: 0.9934 - val_loss: 2.1235 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.69026\n",
      "Epoch 133/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0103 - accuracy: 0.9934 - val_loss: 2.1124 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.69026\n",
      "Epoch 134/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0101 - accuracy: 0.9920 - val_loss: 2.0975 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.69026\n",
      "Epoch 135/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0130 - accuracy: 0.9925 - val_loss: 2.0995 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.69026\n",
      "Epoch 136/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0099 - accuracy: 0.9940 - val_loss: 2.1121 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.69026\n",
      "Epoch 137/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0084 - accuracy: 0.9947 - val_loss: 2.1439 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.69026\n",
      "Epoch 138/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0087 - accuracy: 0.9944 - val_loss: 2.1491 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.69026\n",
      "Epoch 139/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0147 - accuracy: 0.9916 - val_loss: 2.1637 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.69026\n",
      "Epoch 140/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0119 - accuracy: 0.9932 - val_loss: 1.9131 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.69026\n",
      "Epoch 141/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0184 - accuracy: 0.9928 - val_loss: 2.0998 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.69026\n",
      "Epoch 142/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0351 - accuracy: 0.9876 - val_loss: 1.6770 - val_accuracy: 0.7132\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.69026\n",
      "Epoch 143/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0258 - accuracy: 0.9892 - val_loss: 1.7821 - val_accuracy: 0.6615\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.69026\n",
      "Epoch 144/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0155 - accuracy: 0.9917 - val_loss: 1.8244 - val_accuracy: 0.6868\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.69026\n",
      "Epoch 145/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0109 - accuracy: 0.9921 - val_loss: 1.8604 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.69026\n",
      "Epoch 146/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0087 - accuracy: 0.9954 - val_loss: 1.8853 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.69026\n",
      "Epoch 147/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0134 - accuracy: 0.9890 - val_loss: 1.9033 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.69026\n",
      "Epoch 148/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0118 - accuracy: 0.9908 - val_loss: 1.9204 - val_accuracy: 0.6956\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.69026\n",
      "Epoch 149/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0109 - accuracy: 0.9935 - val_loss: 1.9396 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.69026\n",
      "Epoch 150/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0133 - accuracy: 0.9904 - val_loss: 1.9449 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.69026\n",
      "Epoch 151/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0097 - accuracy: 0.9908 - val_loss: 1.9643 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.69026\n",
      "Epoch 152/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0096 - accuracy: 0.9951 - val_loss: 1.9781 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.69026\n",
      "Epoch 153/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0116 - accuracy: 0.9904 - val_loss: 1.9875 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.69026\n",
      "Epoch 154/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0083 - accuracy: 0.9938 - val_loss: 1.9942 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.69026\n",
      "Epoch 155/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0090 - accuracy: 0.9929 - val_loss: 2.0118 - val_accuracy: 0.6989\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.69026\n",
      "Epoch 156/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0121 - accuracy: 0.9929 - val_loss: 2.0238 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.69026\n",
      "Epoch 157/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0108 - accuracy: 0.9900 - val_loss: 2.0291 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.69026\n",
      "Epoch 158/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0106 - accuracy: 0.9932 - val_loss: 2.0399 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.69026\n",
      "Epoch 159/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0109 - accuracy: 0.9928 - val_loss: 2.0412 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.69026\n",
      "Epoch 160/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0109 - accuracy: 0.9911 - val_loss: 2.0571 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.69026\n",
      "Epoch 161/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0091 - accuracy: 0.9957 - val_loss: 2.0595 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.69026\n",
      "Epoch 162/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0105 - accuracy: 0.9959 - val_loss: 2.0666 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.69026\n",
      "Epoch 163/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0109 - accuracy: 0.9914 - val_loss: 2.0715 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.69026\n",
      "Epoch 164/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0102 - accuracy: 0.9918 - val_loss: 2.0843 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.69026\n",
      "Epoch 165/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0129 - accuracy: 0.9907 - val_loss: 2.0925 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.69026\n",
      "Epoch 166/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0112 - accuracy: 0.9891 - val_loss: 2.0996 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.69026\n",
      "Epoch 167/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0091 - accuracy: 0.9935 - val_loss: 2.1018 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.69026\n",
      "Epoch 168/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0109 - accuracy: 0.9944 - val_loss: 2.1051 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.69026\n",
      "Epoch 169/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0125 - accuracy: 0.9935 - val_loss: 2.1103 - val_accuracy: 0.7154\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.69026\n",
      "Epoch 170/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0090 - accuracy: 0.9942 - val_loss: 2.1143 - val_accuracy: 0.7110\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.69026\n",
      "Epoch 171/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0089 - accuracy: 0.9949 - val_loss: 2.1299 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.69026\n",
      "Epoch 172/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0131 - accuracy: 0.9900 - val_loss: 2.1317 - val_accuracy: 0.7099\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.69026\n",
      "Epoch 173/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0112 - accuracy: 0.9925 - val_loss: 2.1333 - val_accuracy: 0.7110\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.69026\n",
      "Epoch 174/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0081 - accuracy: 0.9937 - val_loss: 2.1403 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.69026\n",
      "Epoch 175/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0107 - accuracy: 0.9956 - val_loss: 2.1471 - val_accuracy: 0.7132\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.69026\n",
      "Epoch 176/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0085 - accuracy: 0.9921 - val_loss: 2.1601 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.69026\n",
      "Epoch 177/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0101 - accuracy: 0.9932 - val_loss: 2.1557 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.69026\n",
      "Epoch 178/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0096 - accuracy: 0.9940 - val_loss: 2.1600 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.69026\n",
      "Epoch 179/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0103 - accuracy: 0.9915 - val_loss: 2.1709 - val_accuracy: 0.7176\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.69026\n",
      "Epoch 180/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0098 - accuracy: 0.9921 - val_loss: 2.1766 - val_accuracy: 0.7154\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.69026\n",
      "Epoch 181/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0122 - accuracy: 0.9916 - val_loss: 2.1802 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.69026\n",
      "Epoch 182/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0120 - accuracy: 0.9931 - val_loss: 2.1934 - val_accuracy: 0.7132\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.69026\n",
      "Epoch 183/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0114 - accuracy: 0.9912 - val_loss: 2.1864 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.69026\n",
      "Epoch 184/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0112 - accuracy: 0.9892 - val_loss: 2.1974 - val_accuracy: 0.7176\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.69026\n",
      "Epoch 185/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0091 - accuracy: 0.9937 - val_loss: 2.2043 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.69026\n",
      "Epoch 186/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0119 - accuracy: 0.9927 - val_loss: 2.2087 - val_accuracy: 0.7209\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.69026\n",
      "Epoch 187/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0096 - accuracy: 0.9930 - val_loss: 2.2196 - val_accuracy: 0.7220\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.69026\n",
      "Epoch 188/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0117 - accuracy: 0.9915 - val_loss: 2.2303 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.69026\n",
      "Epoch 189/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0157 - accuracy: 0.9902 - val_loss: 2.2378 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.69026\n",
      "Epoch 190/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0105 - accuracy: 0.9948 - val_loss: 2.2327 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.69026\n",
      "Epoch 191/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0112 - accuracy: 0.9896 - val_loss: 2.2392 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.69026\n",
      "Epoch 192/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0112 - accuracy: 0.9932 - val_loss: 2.2464 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.69026\n",
      "Epoch 193/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0123 - accuracy: 0.9893 - val_loss: 2.2399 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.69026\n",
      "Epoch 194/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0153 - accuracy: 0.9868 - val_loss: 2.2447 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.69026\n",
      "Epoch 195/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0088 - accuracy: 0.9936 - val_loss: 2.2594 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.69026\n",
      "Epoch 196/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0119 - accuracy: 0.9927 - val_loss: 2.2656 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.69026\n",
      "Epoch 197/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0089 - accuracy: 0.9923 - val_loss: 2.2692 - val_accuracy: 0.7220\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.69026\n",
      "Epoch 198/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0119 - accuracy: 0.9934 - val_loss: 2.2805 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.69026\n",
      "Epoch 199/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0112 - accuracy: 0.9937 - val_loss: 2.2726 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.69026\n",
      "Epoch 200/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0114 - accuracy: 0.9914 - val_loss: 2.2892 - val_accuracy: 0.7132\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.69026\n",
      "Epoch 201/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0091 - accuracy: 0.9917 - val_loss: 2.2932 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.69026\n",
      "Epoch 202/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0092 - accuracy: 0.9955 - val_loss: 2.2909 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.69026\n",
      "Epoch 203/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0114 - accuracy: 0.9911 - val_loss: 2.2999 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.69026\n",
      "Epoch 204/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0119 - accuracy: 0.9898 - val_loss: 2.2937 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.69026\n",
      "Epoch 205/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0099 - accuracy: 0.9913 - val_loss: 2.3074 - val_accuracy: 0.7209\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.69026\n",
      "Epoch 206/1500\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.99 - 1s 8ms/step - loss: 0.0108 - accuracy: 0.9930 - val_loss: 2.3018 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.69026\n",
      "Epoch 207/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0103 - accuracy: 0.9952 - val_loss: 2.3053 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.69026\n",
      "Epoch 208/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0112 - accuracy: 0.9907 - val_loss: 2.3092 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.69026\n",
      "Epoch 209/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0088 - accuracy: 0.9948 - val_loss: 2.3236 - val_accuracy: 0.7242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00209: val_loss did not improve from 0.69026\n",
      "Epoch 210/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0122 - accuracy: 0.9917 - val_loss: 2.3287 - val_accuracy: 0.7209\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.69026\n",
      "Epoch 211/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0107 - accuracy: 0.9934 - val_loss: 2.3228 - val_accuracy: 0.7220\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.69026\n",
      "Epoch 212/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0114 - accuracy: 0.9927 - val_loss: 2.3369 - val_accuracy: 0.7220\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.69026\n",
      "Epoch 213/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0113 - accuracy: 0.9904 - val_loss: 2.3407 - val_accuracy: 0.7220\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.69026\n",
      "Epoch 214/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0114 - accuracy: 0.9903 - val_loss: 2.3467 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.69026\n",
      "Epoch 215/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0110 - accuracy: 0.9926 - val_loss: 2.3669 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.69026\n",
      "Epoch 216/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0119 - accuracy: 0.9936 - val_loss: 2.3500 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.69026\n",
      "Epoch 217/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0096 - accuracy: 0.9919 - val_loss: 2.3713 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.69026\n",
      "Epoch 218/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0121 - accuracy: 0.9902 - val_loss: 2.3652 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.69026\n",
      "Epoch 219/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0100 - accuracy: 0.9949 - val_loss: 2.3777 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.69026\n",
      "Epoch 220/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0110 - accuracy: 0.9959 - val_loss: 2.3788 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.69026\n",
      "Epoch 221/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0074 - accuracy: 0.9959 - val_loss: 2.3919 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.69026\n",
      "Epoch 222/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0068 - accuracy: 0.9949 - val_loss: 2.3943 - val_accuracy: 0.7209\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.69026\n",
      "Epoch 223/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0110 - accuracy: 0.9912 - val_loss: 2.3866 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.69026\n",
      "Epoch 224/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0129 - accuracy: 0.9913 - val_loss: 2.3927 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.69026\n",
      "Epoch 225/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0118 - accuracy: 0.9925 - val_loss: 2.4179 - val_accuracy: 0.7176\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.69026\n",
      "Epoch 226/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0120 - accuracy: 0.9905 - val_loss: 2.3879 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.69026\n",
      "Epoch 227/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0105 - accuracy: 0.9955 - val_loss: 2.4101 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.69026\n",
      "Epoch 228/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0111 - accuracy: 0.9948 - val_loss: 2.3947 - val_accuracy: 0.7209\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.69026\n",
      "Epoch 229/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0109 - accuracy: 0.9952 - val_loss: 2.3973 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.69026\n",
      "Epoch 230/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0110 - accuracy: 0.9922 - val_loss: 2.4140 - val_accuracy: 0.7220\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.69026\n",
      "Epoch 231/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0082 - accuracy: 0.9935 - val_loss: 2.4144 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.69026\n",
      "Epoch 232/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0123 - accuracy: 0.9884 - val_loss: 2.4184 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.69026\n",
      "Epoch 233/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0126 - accuracy: 0.9902 - val_loss: 2.4239 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.69026\n",
      "Epoch 234/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0119 - accuracy: 0.9919 - val_loss: 2.4424 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.69026\n",
      "Epoch 235/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0095 - accuracy: 0.9926 - val_loss: 2.4460 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.69026\n",
      "Epoch 236/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9937 - val_loss: 2.4513 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.69026\n",
      "Epoch 237/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0135 - accuracy: 0.9920 - val_loss: 2.4357 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.69026\n",
      "Epoch 238/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9914 - val_loss: 2.4593 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.69026\n",
      "Epoch 239/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0078 - accuracy: 0.9948 - val_loss: 2.4455 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.69026\n",
      "Epoch 240/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0111 - accuracy: 0.9921 - val_loss: 2.4394 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.69026\n",
      "Epoch 241/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0091 - accuracy: 0.9924 - val_loss: 2.4487 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.69026\n",
      "Epoch 242/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0104 - accuracy: 0.9928 - val_loss: 2.4743 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.69026\n",
      "Epoch 243/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0079 - accuracy: 0.9941 - val_loss: 2.4710 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.69026\n",
      "Epoch 244/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0113 - accuracy: 0.9923 - val_loss: 2.4837 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.69026\n",
      "Epoch 245/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0096 - accuracy: 0.9927 - val_loss: 2.4811 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.69026\n",
      "Epoch 246/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0134 - accuracy: 0.9903 - val_loss: 2.4783 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.69026\n",
      "Epoch 247/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0106 - accuracy: 0.9943 - val_loss: 2.4871 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.69026\n",
      "Epoch 248/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0080 - accuracy: 0.9962 - val_loss: 2.4922 - val_accuracy: 0.7319\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.69026\n",
      "Epoch 249/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0091 - accuracy: 0.9935 - val_loss: 2.4978 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.69026\n",
      "Epoch 250/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0152 - accuracy: 0.9876 - val_loss: 2.4907 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.69026\n",
      "Epoch 251/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0076 - accuracy: 0.9937 - val_loss: 2.4954 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.69026\n",
      "Epoch 252/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0119 - accuracy: 0.9916 - val_loss: 2.4812 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.69026\n",
      "Epoch 253/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0078 - accuracy: 0.9947 - val_loss: 2.4883 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.69026\n",
      "Epoch 254/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0082 - accuracy: 0.9957 - val_loss: 2.4835 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.69026\n",
      "Epoch 255/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0116 - accuracy: 0.9904 - val_loss: 2.5040 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.69026\n",
      "Epoch 256/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0120 - accuracy: 0.9894 - val_loss: 2.5064 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.69026\n",
      "Epoch 257/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0114 - accuracy: 0.9904 - val_loss: 2.5407 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.69026\n",
      "Epoch 258/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0093 - accuracy: 0.9932 - val_loss: 2.5065 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.69026\n",
      "Epoch 259/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0109 - accuracy: 0.9928 - val_loss: 2.4963 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.69026\n",
      "Epoch 260/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0129 - accuracy: 0.9940 - val_loss: 2.5235 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.69026\n",
      "Epoch 261/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0113 - accuracy: 0.9895 - val_loss: 2.5006 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.69026\n",
      "Epoch 262/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0099 - accuracy: 0.9947 - val_loss: 2.5131 - val_accuracy: 0.7352\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.69026\n",
      "Epoch 263/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0074 - accuracy: 0.9947 - val_loss: 2.4975 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.69026\n",
      "Epoch 264/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0097 - accuracy: 0.9929 - val_loss: 2.5290 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.69026\n",
      "Epoch 265/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0082 - accuracy: 0.9939 - val_loss: 2.5328 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.69026\n",
      "Epoch 266/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0120 - accuracy: 0.9919 - val_loss: 2.5489 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.69026\n",
      "Epoch 267/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0399 - accuracy: 0.9864 - val_loss: 1.8920 - val_accuracy: 0.6879\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.69026\n",
      "Epoch 268/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0529 - accuracy: 0.9788 - val_loss: 1.7999 - val_accuracy: 0.6791\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.69026\n",
      "Epoch 269/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0166 - accuracy: 0.9934 - val_loss: 1.8525 - val_accuracy: 0.6846\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.69026\n",
      "Epoch 270/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0158 - accuracy: 0.9912 - val_loss: 1.8934 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.69026\n",
      "Epoch 271/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0103 - accuracy: 0.9936 - val_loss: 1.9314 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.69026\n",
      "Epoch 272/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0112 - accuracy: 0.9904 - val_loss: 1.9654 - val_accuracy: 0.6956\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.69026\n",
      "Epoch 273/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0108 - accuracy: 0.9918 - val_loss: 1.9815 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.69026\n",
      "Epoch 274/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0093 - accuracy: 0.9927 - val_loss: 2.0008 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.69026\n",
      "Epoch 275/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0112 - accuracy: 0.9928 - val_loss: 2.0185 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.69026\n",
      "Epoch 276/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0108 - accuracy: 0.9911 - val_loss: 2.0337 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.69026\n",
      "Epoch 277/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0112 - accuracy: 0.9908 - val_loss: 2.0495 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.69026\n",
      "Epoch 278/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0110 - accuracy: 0.9926 - val_loss: 2.0628 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.69026\n",
      "Epoch 279/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0107 - accuracy: 0.9927 - val_loss: 2.0749 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.69026\n",
      "Epoch 280/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0116 - accuracy: 0.9915 - val_loss: 2.0857 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.69026\n",
      "Epoch 281/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0088 - accuracy: 0.9935 - val_loss: 2.1073 - val_accuracy: 0.6956\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.69026\n",
      "Epoch 282/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0147 - accuracy: 0.9895 - val_loss: 2.1110 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.69026\n",
      "Epoch 283/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0116 - accuracy: 0.9934 - val_loss: 2.1175 - val_accuracy: 0.6989\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.69026\n",
      "Epoch 284/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0104 - accuracy: 0.9936 - val_loss: 2.1279 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.69026\n",
      "Epoch 285/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0128 - accuracy: 0.9927 - val_loss: 2.1377 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.69026\n",
      "Epoch 286/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0071 - accuracy: 0.9945 - val_loss: 2.1452 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.69026\n",
      "Epoch 287/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0088 - accuracy: 0.9908 - val_loss: 2.1596 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.69026\n",
      "Epoch 288/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0100 - accuracy: 0.9939 - val_loss: 2.1620 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.69026\n",
      "Epoch 289/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0102 - accuracy: 0.9898 - val_loss: 2.1715 - val_accuracy: 0.7099\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.69026\n",
      "Epoch 290/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0068 - accuracy: 0.9964 - val_loss: 2.1761 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.69026\n",
      "Epoch 291/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0090 - accuracy: 0.9943 - val_loss: 2.1880 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.69026\n",
      "Epoch 292/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0103 - accuracy: 0.9931 - val_loss: 2.1961 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.69026\n",
      "Epoch 293/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0123 - accuracy: 0.9896 - val_loss: 2.2022 - val_accuracy: 0.7099\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.69026\n",
      "Epoch 294/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0161 - accuracy: 0.9872 - val_loss: 2.2097 - val_accuracy: 0.7110\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.69026\n",
      "Epoch 295/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0145 - accuracy: 0.9872 - val_loss: 2.2158 - val_accuracy: 0.7110\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.69026\n",
      "Epoch 296/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0127 - accuracy: 0.9893 - val_loss: 2.2213 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.69026\n",
      "Epoch 297/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0080 - accuracy: 0.9945 - val_loss: 2.2287 - val_accuracy: 0.7110\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.69026\n",
      "Epoch 298/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0150 - accuracy: 0.9876 - val_loss: 2.2331 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.69026\n",
      "Epoch 299/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0121 - accuracy: 0.9939 - val_loss: 2.2442 - val_accuracy: 0.7132\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.69026\n",
      "Epoch 300/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0091 - accuracy: 0.9938 - val_loss: 2.2544 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.69026\n",
      "Epoch 301/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0100 - accuracy: 0.9932 - val_loss: 2.2563 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.69026\n",
      "Epoch 302/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0105 - accuracy: 0.9917 - val_loss: 2.2660 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.69026\n",
      "Epoch 303/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0118 - accuracy: 0.9923 - val_loss: 2.2715 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.69026\n"
     ]
    }
   ],
   "source": [
    "# LSTM 설정\n",
    "model = Sequential()\n",
    "# Embedding( 불러온 단어의 총 개수, 기사당 단어의 수) : 데이터 전처리 과정을 통해, \n",
    "# 입력된 값을 받아 다음 층이 알아들을 수 있는 형태로 변환하는 역할\n",
    "model.add(Embedding( word_size, max_len ) ) \n",
    "model.add(LSTM(49, activation='tanh'))\n",
    "model.add(Dense(5, activation='softmax')) # 다중분류\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile( loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "model_path = 'model/LSTM_0511_2218/{epoch}-{val_loss}.h5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', \n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',patience=300)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_x, \n",
    "                                                     y_encoded, \n",
    "                                                     test_size=0.3, \n",
    "                                                     stratify=y_encoded)\n",
    "\n",
    "\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit( X_train, y_train, validation_split=0.3,\n",
    "                    epochs=1500,\n",
    "                    callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:23:00.459247Z",
     "start_time": "2021-05-11T15:23:00.455257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 43)          129000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 49)                18228     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 250       \n",
      "=================================================================\n",
      "Total params: 147,478\n",
      "Trainable params: 147,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:23:00.594884Z",
     "start_time": "2021-05-11T15:23:00.460243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트셋으로 실험 결과의 오차값을 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋으로 측정한 정확도의 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "# x값을 지정하고 그래프로 확인\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.title('[Figure] LSTM 모델의 결과')\n",
    "plt.plot( x_len, y_vloss, 'o', c='red', markersize=3, label='TestSet Loss')\n",
    "plt.plot(x_len, y_acc, 'o', c='blue', markersize=3, label='TrainSet Acuraccy')\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:23:01.076530Z",
     "start_time": "2021-05-11T15:23:00.596902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.9096\n",
      "0.9096305966377258\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 1.9104 - accuracy: 0.7469\n",
      "0.7469230890274048\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 정확도\n",
    "print(model.evaluate(X_train,y_train)[1])\n",
    "\n",
    "# 테스트 데이터 정확도\n",
    "print(model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:25:40.941870Z",
     "start_time": "2021-05-11T15:23:01.077528Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "67/67 [==============================] - 3s 12ms/step - loss: 1.4039 - accuracy: 0.3620 - val_loss: 1.0281 - val_accuracy: 0.5791\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02806, saving model to model/CNN+LSTM_0511_2218\\1-1.0280647277832031.h5\n",
      "Epoch 2/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.8543 - accuracy: 0.6724 - val_loss: 0.7153 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02806 to 0.71532, saving model to model/CNN+LSTM_0511_2218\\2-0.7153211832046509.h5\n",
      "Epoch 3/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.4999 - accuracy: 0.8357 - val_loss: 0.6773 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71532 to 0.67734, saving model to model/CNN+LSTM_0511_2218\\3-0.6773353219032288.h5\n",
      "Epoch 4/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.3603 - accuracy: 0.8866 - val_loss: 0.7029 - val_accuracy: 0.7769\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.67734\n",
      "Epoch 5/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.2898 - accuracy: 0.9042 - val_loss: 0.7032 - val_accuracy: 0.7978\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.67734\n",
      "Epoch 6/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.2192 - accuracy: 0.9326 - val_loss: 0.7381 - val_accuracy: 0.7934\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.67734\n",
      "Epoch 7/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.2065 - accuracy: 0.9317 - val_loss: 0.8032 - val_accuracy: 0.7736\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.67734\n",
      "Epoch 8/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.1692 - accuracy: 0.9466 - val_loss: 0.8814 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.67734\n",
      "Epoch 9/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.1459 - accuracy: 0.9541 - val_loss: 0.8995 - val_accuracy: 0.7824\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.67734\n",
      "Epoch 10/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.1315 - accuracy: 0.9550 - val_loss: 0.9445 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.67734\n",
      "Epoch 11/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.1098 - accuracy: 0.9626 - val_loss: 1.0119 - val_accuracy: 0.7462\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.67734\n",
      "Epoch 12/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.1054 - accuracy: 0.9626 - val_loss: 1.0247 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.67734\n",
      "Epoch 13/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.1015 - accuracy: 0.9567 - val_loss: 1.0656 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.67734\n",
      "Epoch 14/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0836 - accuracy: 0.9738 - val_loss: 1.0743 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.67734\n",
      "Epoch 15/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0827 - accuracy: 0.9707 - val_loss: 1.0964 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.67734\n",
      "Epoch 16/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0904 - accuracy: 0.9641 - val_loss: 1.1346 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.67734\n",
      "Epoch 17/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0770 - accuracy: 0.9778 - val_loss: 1.1546 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.67734\n",
      "Epoch 18/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0795 - accuracy: 0.9672 - val_loss: 1.1663 - val_accuracy: 0.7604\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.67734\n",
      "Epoch 19/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0674 - accuracy: 0.9773 - val_loss: 1.2424 - val_accuracy: 0.7670\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.67734\n",
      "Epoch 20/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0784 - accuracy: 0.9702 - val_loss: 1.1792 - val_accuracy: 0.7626\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.67734\n",
      "Epoch 21/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0627 - accuracy: 0.9778 - val_loss: 1.2182 - val_accuracy: 0.7681\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.67734\n",
      "Epoch 22/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0758 - accuracy: 0.9696 - val_loss: 1.2524 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.67734\n",
      "Epoch 23/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0754 - accuracy: 0.9724 - val_loss: 1.2533 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.67734\n",
      "Epoch 24/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0664 - accuracy: 0.9724 - val_loss: 1.2599 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.67734\n",
      "Epoch 25/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0645 - accuracy: 0.9746 - val_loss: 1.2872 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.67734\n",
      "Epoch 26/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0629 - accuracy: 0.9752 - val_loss: 1.3109 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.67734\n",
      "Epoch 27/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0728 - accuracy: 0.9737 - val_loss: 1.3007 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.67734\n",
      "Epoch 28/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0634 - accuracy: 0.9751 - val_loss: 1.3035 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.67734\n",
      "Epoch 29/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0542 - accuracy: 0.9789 - val_loss: 1.2759 - val_accuracy: 0.7648\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.67734\n",
      "Epoch 30/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0619 - accuracy: 0.9730 - val_loss: 1.2797 - val_accuracy: 0.7615\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.67734\n",
      "Epoch 31/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0598 - accuracy: 0.9713 - val_loss: 1.3162 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.67734\n",
      "Epoch 32/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0590 - accuracy: 0.9734 - val_loss: 1.3128 - val_accuracy: 0.7593\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.67734\n",
      "Epoch 33/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0518 - accuracy: 0.9776 - val_loss: 1.3112 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.67734\n",
      "Epoch 34/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0703 - accuracy: 0.9699 - val_loss: 1.3787 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.67734\n",
      "Epoch 35/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0654 - accuracy: 0.9713 - val_loss: 1.3802 - val_accuracy: 0.7582\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.67734\n",
      "Epoch 36/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0577 - accuracy: 0.9760 - val_loss: 1.3628 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.67734\n",
      "Epoch 37/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0681 - accuracy: 0.9695 - val_loss: 1.3587 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.67734\n",
      "Epoch 38/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0553 - accuracy: 0.9770 - val_loss: 1.3555 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.67734\n",
      "Epoch 39/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0481 - accuracy: 0.9799 - val_loss: 1.3466 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.67734\n",
      "Epoch 40/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0447 - accuracy: 0.9854 - val_loss: 1.3804 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.67734\n",
      "Epoch 41/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0476 - accuracy: 0.9815 - val_loss: 1.3690 - val_accuracy: 0.7495\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.67734\n",
      "Epoch 42/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0506 - accuracy: 0.9774 - val_loss: 1.4000 - val_accuracy: 0.7549\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.67734\n",
      "Epoch 43/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0464 - accuracy: 0.9820 - val_loss: 1.3753 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.67734\n",
      "Epoch 44/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 1.4101 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.67734\n",
      "Epoch 45/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0577 - accuracy: 0.9733 - val_loss: 1.4096 - val_accuracy: 0.7571\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.67734\n",
      "Epoch 46/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0541 - accuracy: 0.9767 - val_loss: 1.4172 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.67734\n",
      "Epoch 47/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0559 - accuracy: 0.9743 - val_loss: 1.4327 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.67734\n",
      "Epoch 48/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0517 - accuracy: 0.9762 - val_loss: 1.4162 - val_accuracy: 0.7505\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.67734\n",
      "Epoch 49/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0569 - accuracy: 0.9764 - val_loss: 1.4082 - val_accuracy: 0.7527\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.67734\n",
      "Epoch 50/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0551 - accuracy: 0.9741 - val_loss: 1.4317 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.67734\n",
      "Epoch 51/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0509 - accuracy: 0.9784 - val_loss: 1.4397 - val_accuracy: 0.7484\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.67734\n",
      "Epoch 52/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0562 - accuracy: 0.9728 - val_loss: 1.4675 - val_accuracy: 0.7538\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.67734\n",
      "Epoch 53/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0553 - accuracy: 0.9718 - val_loss: 1.4646 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.67734\n",
      "Epoch 54/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0522 - accuracy: 0.9785 - val_loss: 1.4631 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.67734\n",
      "Epoch 55/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0548 - accuracy: 0.9749 - val_loss: 1.4832 - val_accuracy: 0.7516\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.67734\n",
      "Epoch 56/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0555 - accuracy: 0.9712 - val_loss: 1.4555 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.67734\n",
      "Epoch 57/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0519 - accuracy: 0.9761 - val_loss: 1.4844 - val_accuracy: 0.7462\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.67734\n",
      "Epoch 58/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0479 - accuracy: 0.9788 - val_loss: 1.4886 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.67734\n",
      "Epoch 59/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0448 - accuracy: 0.9724 - val_loss: 1.5057 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.67734\n",
      "Epoch 60/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0552 - accuracy: 0.9768 - val_loss: 1.5199 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.67734\n",
      "Epoch 61/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0507 - accuracy: 0.9748 - val_loss: 1.4976 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.67734\n",
      "Epoch 62/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0596 - accuracy: 0.9707 - val_loss: 1.5322 - val_accuracy: 0.7462\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.67734\n",
      "Epoch 63/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0463 - accuracy: 0.9776 - val_loss: 1.5017 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.67734\n",
      "Epoch 64/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0608 - accuracy: 0.9735 - val_loss: 1.5296 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.67734\n",
      "Epoch 65/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0594 - accuracy: 0.9669 - val_loss: 1.5243 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.67734\n",
      "Epoch 66/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0558 - accuracy: 0.9715 - val_loss: 1.5278 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.67734\n",
      "Epoch 67/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0505 - accuracy: 0.9729 - val_loss: 1.5243 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.67734\n",
      "Epoch 68/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0633 - accuracy: 0.9708 - val_loss: 1.5459 - val_accuracy: 0.7418\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.67734\n",
      "Epoch 69/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0500 - accuracy: 0.9764 - val_loss: 1.5176 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.67734\n",
      "Epoch 70/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0548 - accuracy: 0.9768 - val_loss: 1.5319 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.67734\n",
      "Epoch 71/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0427 - accuracy: 0.9828 - val_loss: 1.5551 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.67734\n",
      "Epoch 72/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0520 - accuracy: 0.9786 - val_loss: 1.6243 - val_accuracy: 0.7462\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.67734\n",
      "Epoch 73/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0473 - accuracy: 0.9756 - val_loss: 1.5640 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.67734\n",
      "Epoch 74/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0451 - accuracy: 0.9811 - val_loss: 1.5451 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.67734\n",
      "Epoch 75/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0434 - accuracy: 0.9800 - val_loss: 1.5677 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.67734\n",
      "Epoch 76/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0528 - accuracy: 0.9756 - val_loss: 1.5650 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.67734\n",
      "Epoch 77/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0487 - accuracy: 0.9758 - val_loss: 1.5710 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.67734\n",
      "Epoch 78/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0538 - accuracy: 0.9768 - val_loss: 1.6172 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.67734\n",
      "Epoch 79/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0544 - accuracy: 0.9700 - val_loss: 1.5894 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.67734\n",
      "Epoch 80/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0474 - accuracy: 0.9805 - val_loss: 1.6051 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.67734\n",
      "Epoch 81/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0567 - accuracy: 0.9679 - val_loss: 1.5958 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.67734\n",
      "Epoch 82/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0474 - accuracy: 0.9735 - val_loss: 1.5874 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.67734\n",
      "Epoch 83/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0463 - accuracy: 0.9766 - val_loss: 1.6205 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.67734\n",
      "Epoch 84/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0447 - accuracy: 0.9774 - val_loss: 1.6004 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.67734\n",
      "Epoch 85/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0512 - accuracy: 0.9778 - val_loss: 1.5992 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.67734\n",
      "Epoch 86/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0510 - accuracy: 0.9762 - val_loss: 1.6244 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.67734\n",
      "Epoch 87/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0443 - accuracy: 0.9794 - val_loss: 1.6100 - val_accuracy: 0.7319\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.67734\n",
      "Epoch 88/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0479 - accuracy: 0.9752 - val_loss: 1.6247 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.67734\n",
      "Epoch 89/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0410 - accuracy: 0.9812 - val_loss: 1.6080 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.67734\n",
      "Epoch 90/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0579 - accuracy: 0.9682 - val_loss: 1.6508 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.67734\n",
      "Epoch 91/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0443 - accuracy: 0.9807 - val_loss: 1.5849 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.67734\n",
      "Epoch 92/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0425 - accuracy: 0.9789 - val_loss: 1.6157 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.67734\n",
      "Epoch 93/1500\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.97 - 0s 7ms/step - loss: 0.0547 - accuracy: 0.9730 - val_loss: 1.6290 - val_accuracy: 0.7374\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.67734\n",
      "Epoch 94/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0477 - accuracy: 0.9743 - val_loss: 1.6403 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.67734\n",
      "Epoch 95/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0462 - accuracy: 0.9753 - val_loss: 1.6422 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.67734\n",
      "Epoch 96/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0456 - accuracy: 0.9761 - val_loss: 1.6388 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.67734\n",
      "Epoch 97/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0442 - accuracy: 0.9788 - val_loss: 1.6319 - val_accuracy: 0.7341\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.67734\n",
      "Epoch 98/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0459 - accuracy: 0.9792 - val_loss: 1.6502 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.67734\n",
      "Epoch 99/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0471 - accuracy: 0.9781 - val_loss: 1.6580 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.67734\n",
      "Epoch 100/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0550 - accuracy: 0.9706 - val_loss: 1.6644 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.67734\n",
      "Epoch 101/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0407 - accuracy: 0.9815 - val_loss: 1.6645 - val_accuracy: 0.7319\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.67734\n",
      "Epoch 102/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0502 - accuracy: 0.9770 - val_loss: 1.6517 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.67734\n",
      "Epoch 103/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0492 - accuracy: 0.9775 - val_loss: 1.6692 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.67734\n",
      "Epoch 104/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0543 - accuracy: 0.9714 - val_loss: 1.6696 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.67734\n",
      "Epoch 105/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0440 - accuracy: 0.9797 - val_loss: 1.6862 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.67734\n",
      "Epoch 106/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0494 - accuracy: 0.9768 - val_loss: 1.6791 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.67734\n",
      "Epoch 107/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0427 - accuracy: 0.9786 - val_loss: 1.6934 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.67734\n",
      "Epoch 108/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0492 - accuracy: 0.9770 - val_loss: 1.6707 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.67734\n",
      "Epoch 109/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0498 - accuracy: 0.9773 - val_loss: 1.6994 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.67734\n",
      "Epoch 110/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0479 - accuracy: 0.9794 - val_loss: 1.6891 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.67734\n",
      "Epoch 111/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0470 - accuracy: 0.9744 - val_loss: 1.6920 - val_accuracy: 0.7275\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.67734\n",
      "Epoch 112/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0491 - accuracy: 0.9752 - val_loss: 1.6961 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.67734\n",
      "Epoch 113/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0414 - accuracy: 0.9747 - val_loss: 1.6983 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.67734\n",
      "Epoch 114/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0521 - accuracy: 0.9679 - val_loss: 1.7117 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.67734\n",
      "Epoch 115/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0551 - accuracy: 0.9752 - val_loss: 1.7061 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.67734\n",
      "Epoch 116/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0448 - accuracy: 0.9803 - val_loss: 1.7240 - val_accuracy: 0.7286\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.67734\n",
      "Epoch 117/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0502 - accuracy: 0.9757 - val_loss: 1.7209 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.67734\n",
      "Epoch 118/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0464 - accuracy: 0.9752 - val_loss: 1.7488 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.67734\n",
      "Epoch 119/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0467 - accuracy: 0.9798 - val_loss: 1.7379 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.67734\n",
      "Epoch 120/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0456 - accuracy: 0.9770 - val_loss: 1.7483 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.67734\n",
      "Epoch 121/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0468 - accuracy: 0.9749 - val_loss: 1.7545 - val_accuracy: 0.7264\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.67734\n",
      "Epoch 122/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0425 - accuracy: 0.9796 - val_loss: 1.7499 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.67734\n",
      "Epoch 123/1500\n",
      "67/67 [==============================] - 1s 11ms/step - loss: 0.0445 - accuracy: 0.9781 - val_loss: 1.7562 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.67734\n",
      "Epoch 124/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0489 - accuracy: 0.9750 - val_loss: 1.7766 - val_accuracy: 0.7176\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.67734\n",
      "Epoch 125/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0484 - accuracy: 0.9748 - val_loss: 1.7690 - val_accuracy: 0.7220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00125: val_loss did not improve from 0.67734\n",
      "Epoch 126/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0532 - accuracy: 0.9734 - val_loss: 1.7777 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.67734\n",
      "Epoch 127/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0491 - accuracy: 0.9774 - val_loss: 1.7927 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.67734\n",
      "Epoch 128/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0445 - accuracy: 0.9762 - val_loss: 1.7900 - val_accuracy: 0.7209\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.67734\n",
      "Epoch 129/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0391 - accuracy: 0.9803 - val_loss: 1.8170 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.67734\n",
      "Epoch 130/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0432 - accuracy: 0.9763 - val_loss: 1.8047 - val_accuracy: 0.7242\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.67734\n",
      "Epoch 131/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0521 - accuracy: 0.9764 - val_loss: 1.7968 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.67734\n",
      "Epoch 132/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0475 - accuracy: 0.9769 - val_loss: 1.7994 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.67734\n",
      "Epoch 133/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0477 - accuracy: 0.9779 - val_loss: 1.7967 - val_accuracy: 0.7220\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.67734\n",
      "Epoch 134/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0455 - accuracy: 0.9769 - val_loss: 1.8090 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.67734\n",
      "Epoch 135/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0456 - accuracy: 0.9752 - val_loss: 1.8230 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.67734\n",
      "Epoch 136/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0458 - accuracy: 0.9794 - val_loss: 1.8202 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.67734\n",
      "Epoch 137/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0459 - accuracy: 0.9764 - val_loss: 1.8172 - val_accuracy: 0.7198\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.67734\n",
      "Epoch 138/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0375 - accuracy: 0.9821 - val_loss: 1.8262 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.67734\n",
      "Epoch 139/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0473 - accuracy: 0.9765 - val_loss: 1.8293 - val_accuracy: 0.7187\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.67734\n",
      "Epoch 140/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0507 - accuracy: 0.9747 - val_loss: 1.8320 - val_accuracy: 0.7132\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.67734\n",
      "Epoch 141/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0428 - accuracy: 0.9811 - val_loss: 1.8323 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.67734\n",
      "Epoch 142/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0380 - accuracy: 0.9802 - val_loss: 1.8471 - val_accuracy: 0.7231\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.67734\n",
      "Epoch 143/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0531 - accuracy: 0.9725 - val_loss: 1.8651 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.67734\n",
      "Epoch 144/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0478 - accuracy: 0.9784 - val_loss: 1.8309 - val_accuracy: 0.7176\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.67734\n",
      "Epoch 145/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0463 - accuracy: 0.9763 - val_loss: 1.8576 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.67734\n",
      "Epoch 146/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0368 - accuracy: 0.9787 - val_loss: 1.8600 - val_accuracy: 0.7099\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.67734\n",
      "Epoch 147/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0405 - accuracy: 0.9815 - val_loss: 1.8651 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.67734\n",
      "Epoch 148/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0378 - accuracy: 0.9827 - val_loss: 1.8702 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.67734\n",
      "Epoch 149/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0439 - accuracy: 0.9755 - val_loss: 1.8711 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.67734\n",
      "Epoch 150/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0441 - accuracy: 0.9756 - val_loss: 1.8615 - val_accuracy: 0.7165\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.67734\n",
      "Epoch 151/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0557 - accuracy: 0.9683 - val_loss: 1.8793 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.67734\n",
      "Epoch 152/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0463 - accuracy: 0.9751 - val_loss: 1.8983 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.67734\n",
      "Epoch 153/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0400 - accuracy: 0.9821 - val_loss: 1.8763 - val_accuracy: 0.7121\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.67734\n",
      "Epoch 154/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0469 - accuracy: 0.9785 - val_loss: 1.8873 - val_accuracy: 0.7110\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.67734\n",
      "Epoch 155/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0444 - accuracy: 0.9787 - val_loss: 1.9024 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.67734\n",
      "Epoch 156/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0399 - accuracy: 0.9797 - val_loss: 1.8998 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.67734\n",
      "Epoch 157/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0441 - accuracy: 0.9772 - val_loss: 1.9021 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.67734\n",
      "Epoch 158/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0414 - accuracy: 0.9815 - val_loss: 1.9025 - val_accuracy: 0.7110\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.67734\n",
      "Epoch 159/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0435 - accuracy: 0.9791 - val_loss: 1.9086 - val_accuracy: 0.7099\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.67734\n",
      "Epoch 160/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0472 - accuracy: 0.9764 - val_loss: 1.9177 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.67734\n",
      "Epoch 161/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0553 - accuracy: 0.9682 - val_loss: 1.9163 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.67734\n",
      "Epoch 162/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0442 - accuracy: 0.9766 - val_loss: 1.9234 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.67734\n",
      "Epoch 163/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0516 - accuracy: 0.9740 - val_loss: 1.9132 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.67734\n",
      "Epoch 164/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0374 - accuracy: 0.9808 - val_loss: 1.9317 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.67734\n",
      "Epoch 165/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0441 - accuracy: 0.9802 - val_loss: 1.9244 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.67734\n",
      "Epoch 166/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0495 - accuracy: 0.9760 - val_loss: 1.9196 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.67734\n",
      "Epoch 167/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0406 - accuracy: 0.9787 - val_loss: 1.9298 - val_accuracy: 0.7088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00167: val_loss did not improve from 0.67734\n",
      "Epoch 168/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0516 - accuracy: 0.9752 - val_loss: 1.9428 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.67734\n",
      "Epoch 169/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0483 - accuracy: 0.9758 - val_loss: 1.9469 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.67734\n",
      "Epoch 170/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0408 - accuracy: 0.9816 - val_loss: 1.9387 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.67734\n",
      "Epoch 171/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0429 - accuracy: 0.9817 - val_loss: 1.9464 - val_accuracy: 0.7099\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.67734\n",
      "Epoch 172/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.9755 - val_loss: 1.9542 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.67734\n",
      "Epoch 173/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0457 - accuracy: 0.9764 - val_loss: 1.9737 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.67734\n",
      "Epoch 174/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0384 - accuracy: 0.9806 - val_loss: 1.9590 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.67734\n",
      "Epoch 175/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0471 - accuracy: 0.9752 - val_loss: 1.9589 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.67734\n",
      "Epoch 176/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0408 - accuracy: 0.9805 - val_loss: 1.9709 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.67734\n",
      "Epoch 177/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0429 - accuracy: 0.9789 - val_loss: 1.9496 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.67734\n",
      "Epoch 178/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0518 - accuracy: 0.9750 - val_loss: 1.9615 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.67734\n",
      "Epoch 179/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0563 - accuracy: 0.9699 - val_loss: 1.9706 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.67734\n",
      "Epoch 180/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0394 - accuracy: 0.9829 - val_loss: 1.9780 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.67734\n",
      "Epoch 181/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0472 - accuracy: 0.9716 - val_loss: 1.9833 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.67734\n",
      "Epoch 182/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0468 - accuracy: 0.9757 - val_loss: 1.9828 - val_accuracy: 0.6989\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.67734\n",
      "Epoch 183/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0383 - accuracy: 0.9826 - val_loss: 1.9751 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.67734\n",
      "Epoch 184/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0433 - accuracy: 0.9808 - val_loss: 1.9896 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.67734\n",
      "Epoch 185/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0396 - accuracy: 0.9803 - val_loss: 1.9905 - val_accuracy: 0.7011\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.67734\n",
      "Epoch 186/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0406 - accuracy: 0.9806 - val_loss: 1.9960 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.67734\n",
      "Epoch 187/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0508 - accuracy: 0.9742 - val_loss: 1.9984 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.67734\n",
      "Epoch 188/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0437 - accuracy: 0.9801 - val_loss: 1.9923 - val_accuracy: 0.7011\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.67734\n",
      "Epoch 189/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0473 - accuracy: 0.9749 - val_loss: 1.9991 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.67734\n",
      "Epoch 190/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0444 - accuracy: 0.9796 - val_loss: 1.9981 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.67734\n",
      "Epoch 191/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0358 - accuracy: 0.9837 - val_loss: 1.9997 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.67734\n",
      "Epoch 192/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0452 - accuracy: 0.9770 - val_loss: 2.0085 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.67734\n",
      "Epoch 193/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0394 - accuracy: 0.9832 - val_loss: 2.0113 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.67734\n",
      "Epoch 194/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0458 - accuracy: 0.9804 - val_loss: 2.0061 - val_accuracy: 0.7088\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.67734\n",
      "Epoch 195/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0500 - accuracy: 0.9768 - val_loss: 2.0170 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.67734\n",
      "Epoch 196/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0482 - accuracy: 0.9774 - val_loss: 2.0237 - val_accuracy: 0.7011\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.67734\n",
      "Epoch 197/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0406 - accuracy: 0.9819 - val_loss: 2.0093 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.67734\n",
      "Epoch 198/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0470 - accuracy: 0.9752 - val_loss: 2.0344 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.67734\n",
      "Epoch 199/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0464 - accuracy: 0.9773 - val_loss: 2.0386 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.67734\n",
      "Epoch 200/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0408 - accuracy: 0.9803 - val_loss: 2.0333 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.67734\n",
      "Epoch 201/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0348 - accuracy: 0.9820 - val_loss: 2.0450 - val_accuracy: 0.7011\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.67734\n",
      "Epoch 202/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0531 - accuracy: 0.9691 - val_loss: 2.0395 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.67734\n",
      "Epoch 203/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0427 - accuracy: 0.9796 - val_loss: 2.0418 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.67734\n",
      "Epoch 204/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0538 - accuracy: 0.9693 - val_loss: 2.0367 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.67734\n",
      "Epoch 205/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0521 - accuracy: 0.9762 - val_loss: 2.0411 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.67734\n",
      "Epoch 206/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0447 - accuracy: 0.9792 - val_loss: 2.0356 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.67734\n",
      "Epoch 207/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0493 - accuracy: 0.9736 - val_loss: 2.0387 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.67734\n",
      "Epoch 208/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0459 - accuracy: 0.9763 - val_loss: 2.0441 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.67734\n",
      "Epoch 209/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0532 - accuracy: 0.9702 - val_loss: 2.0374 - val_accuracy: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00209: val_loss did not improve from 0.67734\n",
      "Epoch 210/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0504 - accuracy: 0.9745 - val_loss: 2.0463 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.67734\n",
      "Epoch 211/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0476 - accuracy: 0.9738 - val_loss: 2.0468 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.67734\n",
      "Epoch 212/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0482 - accuracy: 0.9769 - val_loss: 2.0536 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.67734\n",
      "Epoch 213/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0466 - accuracy: 0.9771 - val_loss: 2.0509 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.67734\n",
      "Epoch 214/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0448 - accuracy: 0.9782 - val_loss: 2.0673 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.67734\n",
      "Epoch 215/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0428 - accuracy: 0.9779 - val_loss: 2.0783 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.67734\n",
      "Epoch 216/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0443 - accuracy: 0.9762 - val_loss: 2.0715 - val_accuracy: 0.7077\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.67734\n",
      "Epoch 217/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0456 - accuracy: 0.9774 - val_loss: 2.0577 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.67734\n",
      "Epoch 218/1500\n",
      "67/67 [==============================] - 1s 10ms/step - loss: 0.0493 - accuracy: 0.9743 - val_loss: 2.0728 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.67734\n",
      "Epoch 219/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0461 - accuracy: 0.9739 - val_loss: 2.0763 - val_accuracy: 0.7011\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.67734\n",
      "Epoch 220/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0430 - accuracy: 0.9785 - val_loss: 2.0766 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.67734\n",
      "Epoch 221/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0462 - accuracy: 0.9790 - val_loss: 2.0710 - val_accuracy: 0.6813\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.67734\n",
      "Epoch 222/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0378 - accuracy: 0.9786 - val_loss: 2.0681 - val_accuracy: 0.6692\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.67734\n",
      "Epoch 223/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0485 - accuracy: 0.9779 - val_loss: 1.9504 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.67734\n",
      "Epoch 224/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0547 - accuracy: 0.9726 - val_loss: 1.9105 - val_accuracy: 0.7011\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.67734\n",
      "Epoch 225/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0463 - accuracy: 0.9767 - val_loss: 1.9121 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.67734\n",
      "Epoch 226/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0415 - accuracy: 0.9789 - val_loss: 1.9095 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.67734\n",
      "Epoch 227/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0456 - accuracy: 0.9790 - val_loss: 1.9139 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.67734\n",
      "Epoch 228/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0460 - accuracy: 0.9774 - val_loss: 1.9268 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.67734\n",
      "Epoch 229/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0441 - accuracy: 0.9758 - val_loss: 1.9333 - val_accuracy: 0.7055\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.67734\n",
      "Epoch 230/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0532 - accuracy: 0.9743 - val_loss: 1.9389 - val_accuracy: 0.7066\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.67734\n",
      "Epoch 231/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0506 - accuracy: 0.9746 - val_loss: 1.9458 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.67734\n",
      "Epoch 232/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0461 - accuracy: 0.9749 - val_loss: 1.9565 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.67734\n",
      "Epoch 233/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0483 - accuracy: 0.9765 - val_loss: 1.9586 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.67734\n",
      "Epoch 234/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0498 - accuracy: 0.9708 - val_loss: 1.9624 - val_accuracy: 0.7011\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.67734\n",
      "Epoch 235/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0499 - accuracy: 0.9726 - val_loss: 1.9732 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.67734\n",
      "Epoch 236/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0428 - accuracy: 0.9780 - val_loss: 1.9758 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.67734\n",
      "Epoch 237/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0420 - accuracy: 0.9794 - val_loss: 1.9807 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.67734\n",
      "Epoch 238/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0470 - accuracy: 0.9779 - val_loss: 1.9826 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.67734\n",
      "Epoch 239/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0451 - accuracy: 0.9775 - val_loss: 1.9905 - val_accuracy: 0.6989\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.67734\n",
      "Epoch 240/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0491 - accuracy: 0.9747 - val_loss: 1.9905 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.67734\n",
      "Epoch 241/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0446 - accuracy: 0.9793 - val_loss: 1.9998 - val_accuracy: 0.6989\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.67734\n",
      "Epoch 242/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0467 - accuracy: 0.9800 - val_loss: 2.0001 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.67734\n",
      "Epoch 243/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0449 - accuracy: 0.9765 - val_loss: 2.0085 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.67734\n",
      "Epoch 244/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0377 - accuracy: 0.9850 - val_loss: 2.0086 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.67734\n",
      "Epoch 245/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0436 - accuracy: 0.9805 - val_loss: 2.0168 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.67734\n",
      "Epoch 246/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0494 - accuracy: 0.9765 - val_loss: 2.0220 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.67734\n",
      "Epoch 247/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0443 - accuracy: 0.9791 - val_loss: 2.0245 - val_accuracy: 0.6956\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.67734\n",
      "Epoch 248/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0514 - accuracy: 0.9750 - val_loss: 2.0316 - val_accuracy: 0.6956\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.67734\n",
      "Epoch 249/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0421 - accuracy: 0.9797 - val_loss: 2.0336 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.67734\n",
      "Epoch 250/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0548 - accuracy: 0.9686 - val_loss: 2.0440 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.67734\n",
      "Epoch 251/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0497 - accuracy: 0.9751 - val_loss: 2.0474 - val_accuracy: 0.6912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00251: val_loss did not improve from 0.67734\n",
      "Epoch 252/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0440 - accuracy: 0.9780 - val_loss: 2.0510 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.67734\n",
      "Epoch 253/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0405 - accuracy: 0.9806 - val_loss: 2.0499 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.67734\n",
      "Epoch 254/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0410 - accuracy: 0.9799 - val_loss: 2.0609 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.67734\n",
      "Epoch 255/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0393 - accuracy: 0.9810 - val_loss: 2.0647 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.67734\n",
      "Epoch 256/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0392 - accuracy: 0.9825 - val_loss: 2.0659 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.67734\n",
      "Epoch 257/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0457 - accuracy: 0.9745 - val_loss: 2.0695 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.67734\n",
      "Epoch 258/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0438 - accuracy: 0.9796 - val_loss: 2.0687 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.67734\n",
      "Epoch 259/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0474 - accuracy: 0.9792 - val_loss: 2.0792 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.67734\n",
      "Epoch 260/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0426 - accuracy: 0.9779 - val_loss: 2.0815 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.67734\n",
      "Epoch 261/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0446 - accuracy: 0.9777 - val_loss: 2.0845 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.67734\n",
      "Epoch 262/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0483 - accuracy: 0.9778 - val_loss: 2.0863 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.67734\n",
      "Epoch 263/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0373 - accuracy: 0.9818 - val_loss: 2.0962 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.67734\n",
      "Epoch 264/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0420 - accuracy: 0.9812 - val_loss: 2.0956 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.67734\n",
      "Epoch 265/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0461 - accuracy: 0.9791 - val_loss: 2.0931 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.67734\n",
      "Epoch 266/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0510 - accuracy: 0.9718 - val_loss: 2.1023 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.67734\n",
      "Epoch 267/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0424 - accuracy: 0.9786 - val_loss: 2.1030 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.67734\n",
      "Epoch 268/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0401 - accuracy: 0.9802 - val_loss: 2.1067 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.67734\n",
      "Epoch 269/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.9848 - val_loss: 2.1146 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.67734\n",
      "Epoch 270/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0441 - accuracy: 0.9789 - val_loss: 2.1141 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.67734\n",
      "Epoch 271/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0452 - accuracy: 0.9783 - val_loss: 2.1154 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.67734\n",
      "Epoch 272/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0425 - accuracy: 0.9815 - val_loss: 2.1171 - val_accuracy: 0.6890\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.67734\n",
      "Epoch 273/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0462 - accuracy: 0.9778 - val_loss: 2.1290 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.67734\n",
      "Epoch 274/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0465 - accuracy: 0.9794 - val_loss: 2.1315 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.67734\n",
      "Epoch 275/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0535 - accuracy: 0.9748 - val_loss: 2.1282 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.67734\n",
      "Epoch 276/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0441 - accuracy: 0.9788 - val_loss: 2.1450 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.67734\n",
      "Epoch 277/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0444 - accuracy: 0.9811 - val_loss: 2.1476 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.67734\n",
      "Epoch 278/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0425 - accuracy: 0.9771 - val_loss: 2.1540 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.67734\n",
      "Epoch 279/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0535 - accuracy: 0.9728 - val_loss: 2.1520 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.67734\n",
      "Epoch 280/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0417 - accuracy: 0.9797 - val_loss: 2.1508 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.67734\n",
      "Epoch 281/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0430 - accuracy: 0.9793 - val_loss: 2.1562 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.67734\n",
      "Epoch 282/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0491 - accuracy: 0.9741 - val_loss: 2.1618 - val_accuracy: 0.6934\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.67734\n",
      "Epoch 283/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0401 - accuracy: 0.9804 - val_loss: 2.1665 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.67734\n",
      "Epoch 284/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0460 - accuracy: 0.9782 - val_loss: 2.1582 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.67734\n",
      "Epoch 285/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0342 - accuracy: 0.9831 - val_loss: 2.1554 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.67734\n",
      "Epoch 286/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0417 - accuracy: 0.9841 - val_loss: 2.1723 - val_accuracy: 0.6868\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.67734\n",
      "Epoch 287/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0571 - accuracy: 0.9709 - val_loss: 2.1785 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.67734\n",
      "Epoch 288/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0498 - accuracy: 0.9757 - val_loss: 2.1777 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.67734\n",
      "Epoch 289/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0467 - accuracy: 0.9719 - val_loss: 2.1808 - val_accuracy: 0.6890\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.67734\n",
      "Epoch 290/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0490 - accuracy: 0.9693 - val_loss: 2.1830 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.67734\n",
      "Epoch 291/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0466 - accuracy: 0.9767 - val_loss: 2.1806 - val_accuracy: 0.6890\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.67734\n",
      "Epoch 292/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0438 - accuracy: 0.9746 - val_loss: 2.1831 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.67734\n",
      "Epoch 293/1500\n",
      "67/67 [==============================] - 0s 7ms/step - loss: 0.0506 - accuracy: 0.9764 - val_loss: 2.1919 - val_accuracy: 0.6857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00293: val_loss did not improve from 0.67734\n",
      "Epoch 294/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0447 - accuracy: 0.9759 - val_loss: 2.2026 - val_accuracy: 0.6879\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.67734\n",
      "Epoch 295/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0468 - accuracy: 0.9762 - val_loss: 2.2031 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.67734\n",
      "Epoch 296/1500\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.0412 - accuracy: 0.9798 - val_loss: 2.1987 - val_accuracy: 0.6890\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.67734\n",
      "Epoch 297/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0547 - accuracy: 0.9718 - val_loss: 2.2098 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.67734\n",
      "Epoch 298/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0506 - accuracy: 0.9692 - val_loss: 2.2119 - val_accuracy: 0.6890\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.67734\n",
      "Epoch 299/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0555 - accuracy: 0.9731 - val_loss: 2.2132 - val_accuracy: 0.6879\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.67734\n",
      "Epoch 300/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0446 - accuracy: 0.9786 - val_loss: 2.2232 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.67734\n",
      "Epoch 301/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0426 - accuracy: 0.9782 - val_loss: 2.2185 - val_accuracy: 0.6901\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.67734\n",
      "Epoch 302/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0442 - accuracy: 0.9767 - val_loss: 2.2257 - val_accuracy: 0.6879\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.67734\n",
      "Epoch 303/1500\n",
      "67/67 [==============================] - 1s 8ms/step - loss: 0.0380 - accuracy: 0.9843 - val_loss: 2.2264 - val_accuracy: 0.6879\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.67734\n"
     ]
    }
   ],
   "source": [
    "# CNN + LSTM 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding( word_size, max_len ) )\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(5, activation='softmax')) # 다중분류\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile( loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "model_path = 'model/CNN+LSTM_0511_2218/{epoch}-{val_loss}.h5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_path, monitor='val_loss', \n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss',patience=300)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_x, \n",
    "                                                     y_encoded, \n",
    "                                                     test_size=0.3, \n",
    "                                                     stratify=y_encoded)\n",
    "\n",
    "\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit( X_train, y_train, validation_split=0.3,\n",
    "                    epochs=1500,\n",
    "                    callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:25:40.947856Z",
     "start_time": "2021-05-11T15:25:40.942868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 43)          129000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 64)          13824     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 55)                26400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 280       \n",
      "=================================================================\n",
      "Total params: 169,504\n",
      "Trainable params: 169,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN+LSTM 모델\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:25:41.092470Z",
     "start_time": "2021-05-11T15:25:40.948851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트셋으로 실험 결과의 오차값을 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋으로 측정한 정확도의 값 저장\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "# x값을 지정하고 그래프로 확인\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.title('[Figure] CNN+LSTM 모델의 결과')\n",
    "plt.plot( x_len, y_vloss, 'o', c='red', markersize=3, label='TestSet Loss')\n",
    "plt.plot(x_len, y_acc, 'o', c='blue', markersize=3, label='TrainSet Acuraccy')\n",
    "plt.ylim(0,1.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:25:41.627038Z",
     "start_time": "2021-05-11T15:25:41.095477Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 3ms/step - loss: 0.6997 - accuracy: 0.8908\n",
      "0.8908311128616333\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 2.0208 - accuracy: 0.7015\n",
      "0.7015384435653687\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 정확도\n",
    "print(model.evaluate(X_train,y_train)[1])\n",
    "\n",
    "# 테스트 데이터 정확도\n",
    "print(model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 저장한 모델 불러오기\n",
    "- 1. 테스트 데이터의 정확도가 가장 높고( 0.74) val_loss가 가장 낮은 LSTM 모델 사용\n",
    "- 2. val_loss가 가장 낮은(0.54) DNN도 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:03.088315Z",
     "start_time": "2021-05-11T15:52:02.879329Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = load_model('./model/LSTM_0511_2218/num_words_5000/4-0.6798188090324402.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:33.115296Z",
     "start_time": "2021-05-11T15:52:33.089591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  682,  877,   59],\n",
       "       [   0,    0,    0, ..., 1661,  107,   59],\n",
       "       [   0,    0,    0, ..., 1518, 2362, 4249],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  752,   30,  580],\n",
       "       [   0,    0,    0, ...,  829, 3269,   83],\n",
       "       [   0,    0,    0, ...,    8,  197,  506]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x = pad_sequences(X, 49)\n",
    "padded_x # 배열의 길이가 맞춰짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:35.203334Z",
     "start_time": "2021-05-11T15:52:35.177399Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_x, \n",
    "                                                     y_encoded, \n",
    "                                                     test_size=0.3, \n",
    "                                                     stratify=y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:37.971997Z",
     "start_time": "2021-05-11T15:52:37.243395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 1s 3ms/step - loss: 0.5010 - accuracy: 0.8384\n",
      "0.8383905291557312\n",
      "41/41 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.8500\n",
      "0.8500000238418579\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 정확도\n",
    "print(best_model.evaluate(X_train,y_train)[1])\n",
    "\n",
    "# 테스트 데이터 정확도\n",
    "print(best_model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 학습하지 않은 데이터로 후보자 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:44.508748Z",
     "start_time": "2021-05-11T15:52:44.461876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139776"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하지 않은 데이터의 인덱스 번호\n",
    "null_idx = df2[df2['candidate'].isnull()].index\n",
    "\n",
    "# 예측 대상인 텍스트 데이터 분리\n",
    "docs2 = list(df2.iloc[null_idx]['title_comment'])\n",
    "len(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:49.693173Z",
     "start_time": "2021-05-11T15:52:46.600128Z"
    }
   },
   "outputs": [],
   "source": [
    "# 앞서 만든 토큰의 인덱스로만 채워진 새로운 배열 생성\n",
    "X2 = token.texts_to_sequences(docs2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:51.736994Z",
     "start_time": "2021-05-11T15:52:51.733004Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[785, 68, 70, 36, 1659, 1409, 1006, 959, 145, 1770]\n",
      "재보궐  부산 김영춘  박형준 서울도 양자 대결 철수야 뜸 들이지 말고 애국하는 마음으로 물러서라\n"
     ]
    }
   ],
   "source": [
    "print(X2[0])\n",
    "print(docs2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:53.800593Z",
     "start_time": "2021-05-11T15:52:53.789609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박형준\n",
      "부산\n",
      "김영춘\n",
      "말고\n",
      "재보궐\n",
      "철수야\n",
      "대결\n",
      "양자\n",
      "서울도\n",
      "마음으로\n"
     ]
    }
   ],
   "source": [
    "# 기존 토큰의 인덱스로 채워진 배열의 값 확인\n",
    "for key, val  in  token.word_index.items():\n",
    "    if val in [785, 68, 70, 36, 1659, 1409, 1006, 959, 145, 1770]:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:52:56.449052Z",
     "start_time": "2021-05-11T15:52:55.949859Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139776, 49)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 패딩\n",
    "padded_x2 = pad_sequences(X2, 49)\n",
    "padded_x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:53:06.415400Z",
     "start_time": "2021-05-11T15:52:58.793782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 2, ..., 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측하기\n",
    "y_pred_label = np.argmax(best_model.predict(padded_x2), axis = 1)\n",
    "y_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:53:08.931563Z",
     "start_time": "2021-05-11T15:53:08.922591Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['오세훈', '박영선', '박영선', ..., '오세훈', '오세훈', '오세훈'], dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 디코딩\n",
    "y_pred = encoder.inverse_transform(y_pred_label)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:53:11.451656Z",
     "start_time": "2021-05-11T15:53:11.447057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'박영선'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:53:13.926423Z",
     "start_time": "2021-05-11T15:53:13.920450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'재보궐  부산 김영춘  박형준 서울도 양자 대결 참 답 없네 학생들 밥 주가 싫다고 생떼 부리다 뛰쳐나간 놈을 뽑기도 그래도'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:53:30.710738Z",
     "start_time": "2021-05-11T15:53:30.695778Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "오세훈    91797\n",
       "박영선    29648\n",
       "기타     18157\n",
       "박형준      154\n",
       "김영춘       20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과값 확인\n",
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:53:33.582642Z",
     "start_time": "2021-05-11T15:53:33.571672Z"
    }
   },
   "outputs": [],
   "source": [
    "# 예측한 후보값 넣기\n",
    "df2['area_candidate'][null_idx] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:53:35.754458Z",
     "start_time": "2021-05-11T15:53:35.739496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>candidate</th>\n",
       "      <th>candidate_eval</th>\n",
       "      <th>party</th>\n",
       "      <th>party_eval</th>\n",
       "      <th>title_comment</th>\n",
       "      <th>area_candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 철수야 뜸 들이지 말고 애국하는 마...</td>\n",
       "      <td>오세훈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 박영선은 정동영이 얻은  프로 선에...</td>\n",
       "      <td>박영선</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 빵 산 이가 서울시장 되면 서울은 ...</td>\n",
       "      <td>박영선</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 서울시장 후보 더 든 대치어 없음 ...</td>\n",
       "      <td>박영선</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>재보궐  부산 김영춘  박형준 서울도 양자 대결 부산은 오거돈 선거이고 오거돈 치부...</td>\n",
       "      <td>박영선</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144134</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도직입한 일해져 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해야...</td>\n",
       "      <td>오세훈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144135</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도직입한 일해져 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해야...</td>\n",
       "      <td>오세훈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144136</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도 직입한 일해저 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해...</td>\n",
       "      <td>오세훈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144137</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도 직입한 일해저 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해...</td>\n",
       "      <td>오세훈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144138</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>논설위원의 단도직입한 일해져 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해야...</td>\n",
       "      <td>오세훈</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144139 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        area  candidate  candidate_eval  party  party_eval  \\\n",
       "0        NaN        NaN             NaN    NaN         NaN   \n",
       "1        1.0        1.0             0.0    NaN         NaN   \n",
       "2        1.0        1.0             0.0    NaN         NaN   \n",
       "3        1.0        1.0             0.0    1.0         0.0   \n",
       "4        2.0        NaN             NaN    1.0         0.0   \n",
       "...      ...        ...             ...    ...         ...   \n",
       "144134   NaN        NaN             NaN    NaN         NaN   \n",
       "144135   NaN        NaN             NaN    NaN         NaN   \n",
       "144136   NaN        NaN             NaN    NaN         NaN   \n",
       "144137   NaN        NaN             NaN    NaN         NaN   \n",
       "144138   NaN        NaN             NaN    NaN         NaN   \n",
       "\n",
       "                                            title_comment area_candidate  \n",
       "0       재보궐  부산 김영춘  박형준 서울도 양자 대결 철수야 뜸 들이지 말고 애국하는 마...            오세훈  \n",
       "1       재보궐  부산 김영춘  박형준 서울도 양자 대결 박영선은 정동영이 얻은  프로 선에...            박영선  \n",
       "2       재보궐  부산 김영춘  박형준 서울도 양자 대결 빵 산 이가 서울시장 되면 서울은 ...            박영선  \n",
       "3       재보궐  부산 김영춘  박형준 서울도 양자 대결 서울시장 후보 더 든 대치어 없음 ...            박영선  \n",
       "4       재보궐  부산 김영춘  박형준 서울도 양자 대결 부산은 오거돈 선거이고 오거돈 치부...            박영선  \n",
       "...                                                   ...            ...  \n",
       "144134  논설위원의 단도직입한 일해져 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해야...            오세훈  \n",
       "144135  논설위원의 단도직입한 일해져 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해야...            오세훈  \n",
       "144136  논설위원의 단도 직입한 일해저 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해...            오세훈  \n",
       "144137  논설위원의 단도 직입한 일해저 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해...            오세훈  \n",
       "144138  논설위원의 단도직입한 일해져 터널 경제성 없지만 동북아 경제권 차원서 장기 검토해야...            오세훈  \n",
       "\n",
       "[144139 rows x 7 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T15:54:50.809952Z",
     "start_time": "2021-05-11T15:54:50.016223Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.to_csv('./data/prediction_byModel2/data_byModel2_0512_0054.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.827px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
